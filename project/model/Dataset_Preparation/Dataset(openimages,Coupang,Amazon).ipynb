{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Openimages+Coupang+Amazon 합친 Train/Validation/Test Dataset 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 import (fiftyone)\n",
    "import fiftyone as fo\n",
    "import fiftyone.types as fot\n",
    "import fiftyone.zoo as foz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function fiftyone.core.dataset.list_datasets(glob_patt=None, tags=None, info=False)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋 병합 중 오류 발생시 dataset 이름(고유값) 초기화를 위한 코드\n",
    "fo.delete_datasets(\"\")\n",
    "fo.list_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenImages-v7 (Dog, Cat) 6:3:1 비율로 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading class: Dog\n",
      "Downloading split 'train' to 'D://Data//Fiftyone\\open-images-v7\\train' if necessary\n",
      "Downloading 'https://storage.googleapis.com/openimages/2018_04/train/train-images-boxable-with-rotation.csv' to 'D://Data//Fiftyone\\open-images-v7\\train\\metadata\\image_ids.csv'\n",
      " 100% |██████|    4.8Gb/4.8Gb [17.0s elapsed, 0s remaining, 317.5Mb/s]      \n",
      "Downloading 'https://storage.googleapis.com/openimages/v5/class-descriptions-boxable.csv' to 'D://Data//Fiftyone\\open-images-v7\\train\\metadata\\classes.csv'\n",
      "Downloading 'https://storage.googleapis.com/openimages/2018_04/bbox_labels_600_hierarchy.json' to 'C:\\Users\\jhk16\\AppData\\Local\\Temp\\tmp6m0u8ib3\\metadata\\hierarchy.json'\n",
      "Downloading 'https://storage.googleapis.com/openimages/v6/oidv6-train-annotations-bbox.csv' to 'D://Data//Fiftyone\\open-images-v7\\train\\labels\\detections.csv'\n",
      "Downloading 2700 images\n",
      " 100% |█████████████████| 2700/2700 [1.2m elapsed, 0s remaining, 10.5 files/s]      \n",
      "Dataset info written to 'D://Data//Fiftyone\\open-images-v7\\info.json'\n",
      "Loading 'open-images-v7' split 'train'\n",
      " 100% |███████████████| 2700/2700 [6.0s elapsed, 0s remaining, 435.1 samples/s]      \n",
      "Dataset 'open-images-v7-dog-train' created\n",
      " 100% |███████████████| 2700/2700 [7.8s elapsed, 0s remaining, 345.5 samples/s]      \n",
      "Loading class: Cat\n",
      "Downloading split 'train' to 'D://Data//Fiftyone\\open-images-v7\\train' if necessary\n",
      "Found 11 images, downloading the remaining 2689\n",
      " 100% |█████████████████| 2689/2689 [1.2m elapsed, 0s remaining, 22.0 files/s]      \n",
      "Dataset info written to 'D://Data//Fiftyone\\open-images-v7\\info.json'\n",
      "Loading 'open-images-v7' split 'train'\n",
      " 100% |███████████████| 2700/2700 [3.9s elapsed, 0s remaining, 655.4 samples/s]      \n",
      "Dataset 'open-images-v7-cat-train' created\n",
      " 100% |███████████████| 2700/2700 [5.0s elapsed, 0s remaining, 514.2 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "# 다운르드 받을 class 지정\n",
    "classes = [\n",
    "    \"Dog\", \"Cat\"\n",
    "]\n",
    "\n",
    "# 데이터셋 저장 경로 지정\n",
    "dataset_dir = \"D://Data//Fiftyone\"\n",
    "# dataset_dir 를 fiftyone zoo 다운받을 기본 경로로 지정\n",
    "fo.config.dataset_zoo_dir = dataset_dir\n",
    "\n",
    "# 다운로드 받은 데이터를 저장할 dataset 생성\n",
    "accumulated_dataset = fo.Dataset()\n",
    "\n",
    "for cls in classes:\n",
    "    print(f\"Loading class: {cls}\")\n",
    "\n",
    "    # 각 class에 대한 임시 데이터셋 생성\n",
    "    dataset_name = f\"open-images-v7-{cls.lower().replace(' ', '-')}-train\"\n",
    "\n",
    "    dataset = foz.load_zoo_dataset(\n",
    "        \"open-images-v7\",\n",
    "        split=\"train\",\n",
    "        classes=[cls],\n",
    "        label_types=[\"detections\"],\n",
    "        max_samples=2700, # class 별 고른 분포를 위해 최대 샘플 수 제한\n",
    "        seed=51,\n",
    "        shuffle=True,\n",
    "        dataset_name=dataset_name\n",
    "    )\n",
    "\n",
    "    accumulated_dataset.add_samples(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 5400/5400 [2.3m elapsed, 0s remaining, 48.9 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "# 지정한 class에 대하여 데이터셋 view 생성 (export시 num_label을 원하는 class에 대하여 한정해주기 위해)\n",
    "view = accumulated_dataset.filter_labels(\"ground_truth\", \n",
    "                         fo.ViewField(\"label\").is_in(classes))\n",
    "\n",
    "classes=[\"Dog\", \"Cat\"]\n",
    "view.export(\n",
    "    export_dir=\"D://Data//final/openimages\",  # 원하는 class로 필터링된 COCODetection format openimages train 데이터셋 다운로드 경로\n",
    "    dataset_type=fot.COCODetectionDataset, \n",
    "    classes=classes, # lables.json 안의 category 번호를 원하는 순서대로 지정해주기 위해\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading class: Dog\n",
      "Downloading split 'validation' to 'D://Data//Fiftyone\\open-images-v7\\validation' if necessary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Necessary images already downloaded\n",
      "Existing download of split 'validation' is sufficient\n",
      "Loading 'open-images-v7' split 'validation'\n",
      " 100% |███████████████| 1350/1350 [2.4s elapsed, 0s remaining, 547.7 samples/s]      \n",
      "Dataset 'open-images-v7-dog-val' created\n",
      " 100% |███████████████| 1350/1350 [3.3s elapsed, 0s remaining, 396.0 samples/s]      \n",
      "Loading class: Cat\n",
      "Downloading split 'validation' to 'D://Data//Fiftyone\\open-images-v7\\validation' if necessary\n",
      "Only found 345 (<1350) samples matching your requirements\n",
      "Found 27 images, downloading the remaining 318\n",
      " 100% |███████████████████| 318/318 [11.6s elapsed, 0s remaining, 38.0 files/s]      \n",
      "Dataset info written to 'D://Data//Fiftyone\\open-images-v7\\info.json'\n",
      "Loading 'open-images-v7' split 'validation'\n",
      " 100% |█████████████████| 345/345 [417.6ms elapsed, 0s remaining, 829.5 samples/s]      \n",
      "Dataset 'open-images-v7-cat-val' created\n",
      " 100% |█████████████████| 345/345 [530.0ms elapsed, 0s remaining, 652.8 samples/s]      \n",
      "Directory 'D://Data//final/openimages' already exists; export will be merged with existing files\n",
      " 100% |███████████████| 1695/1695 [17.7s elapsed, 0s remaining, 213.1 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "# 다운르드 받을 class 지정\n",
    "classes = [\n",
    "    \"Dog\", \"Cat\"\n",
    "]\n",
    "\n",
    "# 데이터셋 저장 경로 지정\n",
    "dataset_dir = \"D://Data//Fiftyone\"\n",
    "# dataset_dir 를 fiftyone zoo 다운받을 기본 경로로 지정\n",
    "fo.config.dataset_zoo_dir = dataset_dir\n",
    "\n",
    "# 다운로드 받은 데이터를 저장할 dataset 생성\n",
    "accumulated_dataset = fo.Dataset()\n",
    "\n",
    "for cls in classes:\n",
    "    print(f\"Loading class: {cls}\")\n",
    "\n",
    "    # 각 class에 대한 임시 데이터셋 생성\n",
    "    dataset_name = f\"open-images-v7-{cls.lower().replace(' ', '-')}-val\"\n",
    "\n",
    "    dataset = foz.load_zoo_dataset(\n",
    "        \"open-images-v7\",\n",
    "        split=\"validation\",\n",
    "        classes=[cls],\n",
    "        label_types=[\"detections\"],\n",
    "        max_samples=1350,\n",
    "        seed=51,\n",
    "        shuffle=True,\n",
    "        dataset_name=dataset_name\n",
    "    )\n",
    "\n",
    "    accumulated_dataset.add_samples(dataset)\n",
    "\n",
    "# 지정한 class에 대하여 데이터셋 view 생성 (export시 num_label을 원하는 class에 대하여 한정해주기 위해)\n",
    "view = accumulated_dataset.filter_labels(\"ground_truth\", \n",
    "                         fo.ViewField(\"label\").is_in(classes))\n",
    "\n",
    "classes=[\"Dog\", \"Cat\"]\n",
    "view.export(\n",
    "    export_dir=\"D://Data//final/openimages\",  # 원하는 class로 필터링된 COCODetection format openimages validation 데이터셋 다운로드 경로\n",
    "    dataset_type=fot.COCODetectionDataset, \n",
    "    classes=classes,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading class: Dog\n",
      "Downloading split 'test' to 'D://Data//Fiftyone\\open-images-v7\\test' if necessary\n",
      "Downloading 'https://storage.googleapis.com/openimages/2018_04/test/test-images-with-rotation.csv' to 'D://Data//Fiftyone\\open-images-v7\\test\\metadata\\image_ids.csv'\n",
      "Downloading 'https://storage.googleapis.com/openimages/v5/class-descriptions-boxable.csv' to 'D://Data//Fiftyone\\open-images-v7\\test\\metadata\\classes.csv'\n",
      "Downloading 'https://storage.googleapis.com/openimages/2018_04/bbox_labels_600_hierarchy.json' to 'C:\\Users\\jhk16\\AppData\\Local\\Temp\\tmpl1swba9h\\metadata\\hierarchy.json'\n",
      "Downloading 'https://storage.googleapis.com/openimages/v5/test-annotations-bbox.csv' to 'D://Data//Fiftyone\\open-images-v7\\test\\labels\\detections.csv'\n",
      "Downloading 450 images\n",
      " 100% |███████████████████| 450/450 [13.9s elapsed, 0s remaining, 44.4 files/s]      \n",
      "Dataset info written to 'D://Data//Fiftyone\\open-images-v7\\info.json'\n",
      "Loading 'open-images-v7' split 'test'\n",
      " 100% |█████████████████| 450/450 [748.7ms elapsed, 0s remaining, 602.4 samples/s]      \n",
      "Dataset 'open-images-v7-dog-test' created\n",
      " 100% |█████████████████| 450/450 [1.1s elapsed, 0s remaining, 412.4 samples/s]         \n",
      "Loading class: Cat\n",
      "Downloading split 'test' to 'D://Data//Fiftyone\\open-images-v7\\test' if necessary\n",
      "Downloading 450 images\n",
      " 100% |███████████████████| 450/450 [14.0s elapsed, 0s remaining, 43.8 files/s]      \n",
      "Dataset info written to 'D://Data//Fiftyone\\open-images-v7\\info.json'\n",
      "Loading 'open-images-v7' split 'test'\n",
      " 100% |█████████████████| 450/450 [506.1ms elapsed, 0s remaining, 889.1 samples/s]      \n",
      "Dataset 'open-images-v7-cat-test' created\n",
      " 100% |█████████████████| 450/450 [637.6ms elapsed, 0s remaining, 705.7 samples/s]      \n",
      "Directory 'D://Data//final/openimages' already exists; export will be merged with existing files\n",
      " 100% |█████████████████| 900/900 [4.4s elapsed, 0s remaining, 217.7 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "# 다운르드 받을 class 지정\n",
    "classes = [\n",
    "    \"Dog\", \"Cat\"\n",
    "]\n",
    "\n",
    "# 데이터셋 저장 경로 지정\n",
    "dataset_dir = \"D://Data//Fiftyone\"\n",
    "# dataset_dir 를 fiftyone zoo 다운받을 기본 경로로 지정\n",
    "fo.config.dataset_zoo_dir = dataset_dir\n",
    "\n",
    "# 다운로드 받은 데이터를 저장할 dataset 생성\n",
    "accumulated_dataset = fo.Dataset()\n",
    "\n",
    "for cls in classes:\n",
    "    print(f\"Loading class: {cls}\")\n",
    "\n",
    "    # 각 class에 대한 임시 데이터셋 생성\n",
    "    dataset_name = f\"open-images-v7-{cls.lower().replace(' ', '-')}-test\"\n",
    "\n",
    "    dataset = foz.load_zoo_dataset(\n",
    "        \"open-images-v7\",\n",
    "        split=\"test\",\n",
    "        classes=[cls],\n",
    "        label_types=[\"detections\"],\n",
    "        max_samples=450,\n",
    "        seed=51,\n",
    "        shuffle=True,\n",
    "        dataset_name=dataset_name\n",
    "    )\n",
    "\n",
    "    accumulated_dataset.add_samples(dataset)\n",
    "\n",
    "# 지정한 class에 대하여 데이터셋 view 생성 (export시 num_label을 원하는 class에 대하여 한정해주기 위해)\n",
    "view = accumulated_dataset.filter_labels(\"ground_truth\", \n",
    "                         fo.ViewField(\"label\").is_in(classes))\n",
    "\n",
    "classes=[\"Dog\", \"Cat\"]\n",
    "view.export(\n",
    "    export_dir=\"D://Data//final/openimages\",  # 원하는 class로 필터링된 COCODetection format openimages validation 데이터셋 다운로드 경로\n",
    "    dataset_type=fot.COCODetectionDataset, \n",
    "    classes=classes,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coupang 데이터셋 각 class 별로 불러와서 다 합친 후 6:3:1로 재분배"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 2_dog_fence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 1022/1022 [999.8ms elapsed, 0s remaining, 1.0K samples/s]       \n",
      " 100% |█████████████████| 130/130 [122.5ms elapsed, 0s remaining, 1.1K samples/s]    \n",
      " 100% |█████████████████| 116/116 [125.3ms elapsed, 0s remaining, 938.5 samples/s] \n",
      "Name:        original_dogfence_train\n",
      "Media type:  image\n",
      "Num samples: 1022\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:         fiftyone.core.fields.ObjectIdField\n",
      "    filepath:   fiftyone.core.fields.StringField\n",
      "    tags:       fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    detections: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "Name:        original_dogfence_val\n",
      "Media type:  image\n",
      "Num samples: 130\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:         fiftyone.core.fields.ObjectIdField\n",
      "    filepath:   fiftyone.core.fields.StringField\n",
      "    tags:       fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    detections: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "Name:        original_dogfence_test\n",
      "Media type:  image\n",
      "Num samples: 116\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:         fiftyone.core.fields.ObjectIdField\n",
      "    filepath:   fiftyone.core.fields.StringField\n",
      "    tags:       fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    detections: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      " 100% |███████████████| 1022/1022 [1.3s elapsed, 0s remaining, 785.1 samples/s]         \n",
      " 100% |█████████████████| 130/130 [157.0ms elapsed, 0s remaining, 828.2 samples/s]    \n",
      " 100% |█████████████████| 116/116 [149.5ms elapsed, 0s remaining, 783.8 samples/s]    \n",
      " 100% |█████████████████| 760/760 [12.6s elapsed, 0s remaining, 58.9 samples/s]      \n",
      " 100% |█████████████████| 380/380 [3.4s elapsed, 0s remaining, 103.6 samples/s]      \n",
      " 100% |█████████████████| 128/128 [451.6ms elapsed, 0s remaining, 283.5 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fiftyone as fo\n",
    "import fiftyone.types as fot\n",
    "\n",
    "#기존 데이터셋 로드\n",
    "original_dataset_dir = f\"D:/Data/roboflow/2_dog_fence/\"\n",
    "original_train_dataset_dir = os.path.join(original_dataset_dir, \"train\")\n",
    "original_val_dataset_dir = os.path.join(original_dataset_dir, \"val\")\n",
    "original_test_dataset_dir = os.path.join(original_dataset_dir, \"test\")\n",
    "\n",
    "original_train_dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=original_train_dataset_dir,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    name = \"original_dogfence_train\"\n",
    ")\n",
    "\n",
    "original_val_dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=original_val_dataset_dir,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    name = \"original_dogfence_val\"\n",
    ")\n",
    "\n",
    "original_test_dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=original_test_dataset_dir,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    name = \"original_dogfence_test\"\n",
    ")\n",
    "\n",
    "print(original_train_dataset)\n",
    "print(original_val_dataset)\n",
    "print(original_test_dataset)\n",
    "\n",
    "# 기존 데이터셋 하나로 합쳐서 섞기\n",
    "combined_dataset = fo.Dataset(\"combined_dogfence\")\n",
    "\n",
    "combined_dataset.add_samples(original_train_dataset)\n",
    "combined_dataset.add_samples(original_val_dataset)\n",
    "combined_dataset.add_samples(original_test_dataset)\n",
    "\n",
    "combined_dataset.shuffle(seed=41)\n",
    "\n",
    "# train:val:test = 6:3:1 비율로 나누기\n",
    "num_combined_samples = len(combined_dataset)\n",
    "num_train = int(num_combined_samples * 0.6)\n",
    "num_val = int(num_combined_samples * 0.3)\n",
    "\n",
    "new_train_samples = combined_dataset.take(num_train)\n",
    "new_val_samples = combined_dataset.skip(num_train).take(num_val)\n",
    "new_test_samples = combined_dataset.skip(num_train + num_val)\n",
    "\n",
    "# 나눈 sample에 각각 train, val, test 태그 달기\n",
    "for sample in new_train_samples:\n",
    "    sample.tags = [\"train\"]\n",
    "    sample.save()\n",
    "\n",
    "for sample in new_val_samples:\n",
    "    sample.tags = [\"val\"]\n",
    "    sample.save()\n",
    "\n",
    "for sample in new_test_samples:\n",
    "    sample.tags = [\"test\"]\n",
    "    sample.save()\n",
    "\n",
    "# classes 지정\n",
    "classes = ['dog_fence']\n",
    "\n",
    "# 새로운 train, val, test 데이터셋 export\n",
    "new_train_samples.export(\n",
    "    export_dir = f\"D:/Data/roboflow/631_dataset/2_dog_fence/train\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")\n",
    "\n",
    "new_val_samples.export(\n",
    "    export_dir = f\"D:/Data/roboflow/631_dataset/2_dog_fence/val\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")\n",
    "\n",
    "new_test_samples.export(\n",
    "    export_dir = f\"D:/Data/roboflow/631_dataset/2_dog_fence/test\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 3_dog_nosework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 1055/1055 [996.0ms elapsed, 0s remaining, 1.1K samples/s]       \n",
      " 100% |█████████████████| 132/132 [121.7ms elapsed, 0s remaining, 1.1K samples/s]    \n",
      " 100% |█████████████████| 132/132 [134.8ms elapsed, 0s remaining, 979.2 samples/s]   \n",
      "Name:        original_dognosework_train\n",
      "Media type:  image\n",
      "Num samples: 1055\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:         fiftyone.core.fields.ObjectIdField\n",
      "    filepath:   fiftyone.core.fields.StringField\n",
      "    tags:       fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    detections: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "Name:        original_dognosework_val\n",
      "Media type:  image\n",
      "Num samples: 132\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:         fiftyone.core.fields.ObjectIdField\n",
      "    filepath:   fiftyone.core.fields.StringField\n",
      "    tags:       fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    detections: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "Name:        original_dognosework_test\n",
      "Media type:  image\n",
      "Num samples: 132\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:         fiftyone.core.fields.ObjectIdField\n",
      "    filepath:   fiftyone.core.fields.StringField\n",
      "    tags:       fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    detections: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      " 100% |███████████████| 1055/1055 [1.2s elapsed, 0s remaining, 885.2 samples/s]         \n",
      " 100% |█████████████████| 132/132 [278.2ms elapsed, 0s remaining, 474.5 samples/s]     \n",
      " 100% |█████████████████| 132/132 [163.2ms elapsed, 0s remaining, 808.6 samples/s]    \n",
      " 100% |█████████████████| 791/791 [11.7s elapsed, 0s remaining, 62.6 samples/s]      \n",
      " 100% |█████████████████| 395/395 [2.8s elapsed, 0s remaining, 140.6 samples/s]      \n",
      " 100% |█████████████████| 133/133 [487.0ms elapsed, 0s remaining, 273.1 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fiftyone as fo\n",
    "import fiftyone.types as fot\n",
    "\n",
    "#기존 데이터셋 로드\n",
    "original_dataset_dir = f\"D:/Data/roboflow/3_dog_nosework/\"\n",
    "original_train_dataset_dir = os.path.join(original_dataset_dir, \"train\")\n",
    "original_val_dataset_dir = os.path.join(original_dataset_dir, \"val\")\n",
    "original_test_dataset_dir = os.path.join(original_dataset_dir, \"test\")\n",
    "\n",
    "original_train_dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=original_train_dataset_dir,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    name = \"original_dognosework_train\"\n",
    ")\n",
    "\n",
    "original_val_dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=original_val_dataset_dir,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    name = \"original_dognosework_val\"\n",
    ")\n",
    "\n",
    "original_test_dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=original_test_dataset_dir,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    name = \"original_dognosework_test\"\n",
    ")\n",
    "\n",
    "print(original_train_dataset)\n",
    "print(original_val_dataset)\n",
    "print(original_test_dataset)\n",
    "\n",
    "# 기존 데이터셋 하나로 합쳐서 섞기\n",
    "combined_dataset = fo.Dataset(\"combined_dognosework\")\n",
    "\n",
    "combined_dataset.add_samples(original_train_dataset)\n",
    "combined_dataset.add_samples(original_val_dataset)\n",
    "combined_dataset.add_samples(original_test_dataset)\n",
    "\n",
    "combined_dataset.shuffle(seed=41)\n",
    "\n",
    "# train:val:test = 6:3:1 비율로 나누기\n",
    "num_combined_samples = len(combined_dataset)\n",
    "num_train = int(num_combined_samples * 0.6)\n",
    "num_val = int(num_combined_samples * 0.3)\n",
    "\n",
    "new_train_samples = combined_dataset.take(num_train)\n",
    "new_val_samples = combined_dataset.skip(num_train).take(num_val)\n",
    "new_test_samples = combined_dataset.skip(num_train + num_val)\n",
    "\n",
    "# 나눈 sample에 각각 train, val, test 태그 달기\n",
    "for sample in new_train_samples:\n",
    "    sample.tags = [\"train\"]\n",
    "    sample.save()\n",
    "\n",
    "for sample in new_val_samples:\n",
    "    sample.tags = [\"val\"]\n",
    "    sample.save()\n",
    "\n",
    "for sample in new_test_samples:\n",
    "    sample.tags = [\"test\"]\n",
    "    sample.save()\n",
    "\n",
    "# classes 지정\n",
    "classes = ['dog_nosework']\n",
    "\n",
    "# 새로운 train, val, test 데이터셋 export\n",
    "new_train_samples.export(\n",
    "    export_dir = f\"D:/Data/roboflow/631_dataset/3_dog_nosework/train\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")\n",
    "\n",
    "new_val_samples.export(\n",
    "    export_dir = f\"D:/Data/roboflow/631_dataset/3_dog_nosework/val\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")\n",
    "\n",
    "new_test_samples.export(\n",
    "    export_dir = f\"D:/Data/roboflow/631_dataset/3_dog_nosework/test\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 4_cat_tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 970/970 [668.4ms elapsed, 0s remaining, 1.5K samples/s]      \n",
      " 100% |█████████████████| 122/122 [95.5ms elapsed, 0s remaining, 1.3K samples/s]      \n",
      " 100% |█████████████████| 121/121 [103.3ms elapsed, 0s remaining, 1.2K samples/s]  \n",
      "Name:        original_cattower_train\n",
      "Media type:  image\n",
      "Num samples: 970\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:         fiftyone.core.fields.ObjectIdField\n",
      "    filepath:   fiftyone.core.fields.StringField\n",
      "    tags:       fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    detections: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "Name:        original_cattower_val\n",
      "Media type:  image\n",
      "Num samples: 122\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:         fiftyone.core.fields.ObjectIdField\n",
      "    filepath:   fiftyone.core.fields.StringField\n",
      "    tags:       fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    detections: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "Name:        original_cattower_test\n",
      "Media type:  image\n",
      "Num samples: 121\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:         fiftyone.core.fields.ObjectIdField\n",
      "    filepath:   fiftyone.core.fields.StringField\n",
      "    tags:       fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    detections: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      " 100% |█████████████████| 970/970 [925.9ms elapsed, 0s remaining, 1.0K samples/s]       \n",
      " 100% |█████████████████| 122/122 [236.9ms elapsed, 0s remaining, 518.3 samples/s]  \n",
      " 100% |█████████████████| 121/121 [123.0ms elapsed, 0s remaining, 994.9 samples/s]    \n",
      " 100% |█████████████████| 727/727 [7.1s elapsed, 0s remaining, 94.6 samples/s]       \n",
      " 100% |█████████████████| 363/363 [1.9s elapsed, 0s remaining, 226.3 samples/s]      \n",
      " 100% |█████████████████| 123/123 [396.6ms elapsed, 0s remaining, 310.1 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fiftyone as fo\n",
    "import fiftyone.types as fot\n",
    "\n",
    "#기존 데이터셋 로드\n",
    "original_dataset_dir = f\"D:/Data/roboflow/4_cat_tower/\"\n",
    "original_train_dataset_dir = os.path.join(original_dataset_dir, \"train\")\n",
    "original_val_dataset_dir = os.path.join(original_dataset_dir, \"val\")\n",
    "original_test_dataset_dir = os.path.join(original_dataset_dir, \"test\")\n",
    "\n",
    "original_train_dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=original_train_dataset_dir,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    name = \"original_cattower_train\"\n",
    ")\n",
    "\n",
    "original_val_dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=original_val_dataset_dir,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    name = \"original_cattower_val\"\n",
    ")\n",
    "\n",
    "original_test_dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=original_test_dataset_dir,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    name = \"original_cattower_test\"\n",
    ")\n",
    "\n",
    "print(original_train_dataset)\n",
    "print(original_val_dataset)\n",
    "print(original_test_dataset)\n",
    "\n",
    "# 기존 데이터셋 하나로 합쳐서 섞기\n",
    "combined_dataset = fo.Dataset(\"combined_cattower\")\n",
    "\n",
    "combined_dataset.add_samples(original_train_dataset)\n",
    "combined_dataset.add_samples(original_val_dataset)\n",
    "combined_dataset.add_samples(original_test_dataset)\n",
    "\n",
    "combined_dataset.shuffle(seed=41)\n",
    "\n",
    "# train:val:test = 6:3:1 비율로 나누기\n",
    "num_combined_samples = len(combined_dataset)\n",
    "num_train = int(num_combined_samples * 0.6)\n",
    "num_val = int(num_combined_samples * 0.3)\n",
    "\n",
    "new_train_samples = combined_dataset.take(num_train)\n",
    "new_val_samples = combined_dataset.skip(num_train).take(num_val)\n",
    "new_test_samples = combined_dataset.skip(num_train + num_val)\n",
    "\n",
    "# 나눈 sample에 각각 train, val, test 태그 달기\n",
    "for sample in new_train_samples:\n",
    "    sample.tags = [\"train\"]\n",
    "    sample.save()\n",
    "\n",
    "for sample in new_val_samples:\n",
    "    sample.tags = [\"val\"]\n",
    "    sample.save()\n",
    "\n",
    "for sample in new_test_samples:\n",
    "    sample.tags = [\"test\"]\n",
    "    sample.save()\n",
    "\n",
    "# classes 지정\n",
    "classes = ['cat_tower']\n",
    "\n",
    "# 새로운 train, val, test 데이터셋 export\n",
    "new_train_samples.export(\n",
    "    export_dir = f\"D:/Data/roboflow/631_dataset/4_cat_tower/train\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")\n",
    "\n",
    "new_val_samples.export(\n",
    "    export_dir = f\"D:/Data/roboflow/631_dataset/4_cat_tower/val\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")\n",
    "\n",
    "new_test_samples.export(\n",
    "    export_dir = f\"D:/Data/roboflow/631_dataset/4_cat_tower/test\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) 5_cat_scratcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 1800/1800 [1.6s elapsed, 0s remaining, 1.2K samples/s]         \n",
      " 100% |█████████████████| 224/224 [192.2ms elapsed, 0s remaining, 1.2K samples/s]     \n",
      " 100% |█████████████████| 234/234 [195.0ms elapsed, 0s remaining, 1.2K samples/s]     \n",
      "Name:        original_catscratcher_train\n",
      "Media type:  image\n",
      "Num samples: 1800\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:         fiftyone.core.fields.ObjectIdField\n",
      "    filepath:   fiftyone.core.fields.StringField\n",
      "    tags:       fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    detections: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "Name:        original_catscratcher_val\n",
      "Media type:  image\n",
      "Num samples: 224\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:         fiftyone.core.fields.ObjectIdField\n",
      "    filepath:   fiftyone.core.fields.StringField\n",
      "    tags:       fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    detections: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "Name:        original_catscratcher_test\n",
      "Media type:  image\n",
      "Num samples: 234\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:         fiftyone.core.fields.ObjectIdField\n",
      "    filepath:   fiftyone.core.fields.StringField\n",
      "    tags:       fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    detections: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      " 100% |███████████████| 1800/1800 [2.0s elapsed, 0s remaining, 880.7 samples/s]      \n",
      " 100% |█████████████████| 224/224 [249.6ms elapsed, 0s remaining, 902.5 samples/s]      \n",
      " 100% |█████████████████| 234/234 [250.2ms elapsed, 0s remaining, 941.1 samples/s]      \n",
      " 100% |███████████████| 1354/1354 [20.9s elapsed, 0s remaining, 72.1 samples/s]      \n",
      " 100% |█████████████████| 677/677 [4.6s elapsed, 0s remaining, 137.6 samples/s]      \n",
      " 100% |█████████████████| 227/227 [564.3ms elapsed, 0s remaining, 402.3 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fiftyone as fo\n",
    "import fiftyone.types as fot\n",
    "\n",
    "#기존 데이터셋 로드\n",
    "original_dataset_dir = f\"D:/Data/roboflow/5_cat_scratcher/\"\n",
    "original_train_dataset_dir = os.path.join(original_dataset_dir, \"train\")\n",
    "original_val_dataset_dir = os.path.join(original_dataset_dir, \"val\")\n",
    "original_test_dataset_dir = os.path.join(original_dataset_dir, \"test\")\n",
    "\n",
    "original_train_dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=original_train_dataset_dir,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    name = \"original_catscratcher_train\"\n",
    ")\n",
    "\n",
    "original_val_dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=original_val_dataset_dir,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    name = \"original_catscratcher_val\"\n",
    ")\n",
    "\n",
    "original_test_dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=original_test_dataset_dir,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    name = \"original_catscratcher_test\"\n",
    ")\n",
    "\n",
    "print(original_train_dataset)\n",
    "print(original_val_dataset)\n",
    "print(original_test_dataset)\n",
    "\n",
    "# 기존 데이터셋 하나로 합쳐서 섞기\n",
    "combined_dataset = fo.Dataset(\"combined_catscratcher\")\n",
    "\n",
    "combined_dataset.add_samples(original_train_dataset)\n",
    "combined_dataset.add_samples(original_val_dataset)\n",
    "combined_dataset.add_samples(original_test_dataset)\n",
    "\n",
    "combined_dataset.shuffle(seed=41)\n",
    "\n",
    "# train:val:test = 6:3:1 비율로 나누기\n",
    "num_combined_samples = len(combined_dataset)\n",
    "num_train = int(num_combined_samples * 0.6)\n",
    "num_val = int(num_combined_samples * 0.3)\n",
    "\n",
    "new_train_samples = combined_dataset.take(num_train)\n",
    "new_val_samples = combined_dataset.skip(num_train).take(num_val)\n",
    "new_test_samples = combined_dataset.skip(num_train + num_val)\n",
    "\n",
    "# 나눈 sample에 각각 train, val, test 태그 달기\n",
    "for sample in new_train_samples:\n",
    "    sample.tags = [\"train\"]\n",
    "    sample.save()\n",
    "\n",
    "for sample in new_val_samples:\n",
    "    sample.tags = [\"val\"]\n",
    "    sample.save()\n",
    "\n",
    "for sample in new_test_samples:\n",
    "    sample.tags = [\"test\"]\n",
    "    sample.save()\n",
    "\n",
    "# classes 지정\n",
    "classes = ['cat_scratcher']\n",
    "\n",
    "# 새로운 train, val, test 데이터셋 export\n",
    "new_train_samples.export(\n",
    "    export_dir = f\"D:/Data/roboflow/631_dataset/5_cat_scratcher/train\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")\n",
    "\n",
    "new_val_samples.export(\n",
    "    export_dir = f\"D:/Data/roboflow/631_dataset/5_cat_scratcher/val\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")\n",
    "\n",
    "new_test_samples.export(\n",
    "    export_dir = f\"D:/Data/roboflow/631_dataset/5_cat_scratcher/test\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) 6_cat_toilet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 1869/1869 [1.6s elapsed, 0s remaining, 1.2K samples/s]         \n",
      " 100% |█████████████████| 237/237 [193.7ms elapsed, 0s remaining, 1.2K samples/s]  \n",
      " 100% |█████████████████| 238/238 [197.4ms elapsed, 0s remaining, 1.2K samples/s]  \n",
      "Name:        original_cattoilet_train\n",
      "Media type:  image\n",
      "Num samples: 1869\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:         fiftyone.core.fields.ObjectIdField\n",
      "    filepath:   fiftyone.core.fields.StringField\n",
      "    tags:       fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    detections: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "Name:        original_cattoilet_val\n",
      "Media type:  image\n",
      "Num samples: 237\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:         fiftyone.core.fields.ObjectIdField\n",
      "    filepath:   fiftyone.core.fields.StringField\n",
      "    tags:       fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    detections: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "Name:        original_cattoilet_test\n",
      "Media type:  image\n",
      "Num samples: 238\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:         fiftyone.core.fields.ObjectIdField\n",
      "    filepath:   fiftyone.core.fields.StringField\n",
      "    tags:       fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    detections: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      " 100% |███████████████| 1869/1869 [2.1s elapsed, 0s remaining, 890.6 samples/s]      \n",
      " 100% |█████████████████| 237/237 [257.7ms elapsed, 0s remaining, 924.0 samples/s]      \n",
      " 100% |█████████████████| 238/238 [250.0ms elapsed, 0s remaining, 957.6 samples/s]      \n",
      " 100% |███████████████| 1406/1406 [20.2s elapsed, 0s remaining, 68.4 samples/s]      \n",
      " 100% |█████████████████| 703/703 [5.1s elapsed, 0s remaining, 155.1 samples/s]      \n",
      " 100% |█████████████████| 235/235 [682.7ms elapsed, 0s remaining, 344.2 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fiftyone as fo\n",
    "import fiftyone.types as fot\n",
    "\n",
    "#기존 데이터셋 로드\n",
    "original_dataset_dir = f\"D:/Data/roboflow/6_cat_toilet/\"\n",
    "original_train_dataset_dir = os.path.join(original_dataset_dir, \"train\")\n",
    "original_val_dataset_dir = os.path.join(original_dataset_dir, \"val\")\n",
    "original_test_dataset_dir = os.path.join(original_dataset_dir, \"test\")\n",
    "\n",
    "original_train_dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=original_train_dataset_dir,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    name = \"original_cattoilet_train\"\n",
    ")\n",
    "\n",
    "original_val_dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=original_val_dataset_dir,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    name = \"original_cattoilet_val\"\n",
    ")\n",
    "\n",
    "original_test_dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=original_test_dataset_dir,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    name = \"original_cattoilet_test\"\n",
    ")\n",
    "\n",
    "print(original_train_dataset)\n",
    "print(original_val_dataset)\n",
    "print(original_test_dataset)\n",
    "\n",
    "# 기존 데이터셋 하나로 합쳐서 섞기\n",
    "combined_dataset = fo.Dataset(\"combined_cattoilet\")\n",
    "\n",
    "combined_dataset.add_samples(original_train_dataset)\n",
    "combined_dataset.add_samples(original_val_dataset)\n",
    "combined_dataset.add_samples(original_test_dataset)\n",
    "\n",
    "combined_dataset.shuffle(seed=41)\n",
    "\n",
    "# train:val:test = 6:3:1 비율로 나누기\n",
    "num_combined_samples = len(combined_dataset)\n",
    "num_train = int(num_combined_samples * 0.6)\n",
    "num_val = int(num_combined_samples * 0.3)\n",
    "\n",
    "new_train_samples = combined_dataset.take(num_train)\n",
    "new_val_samples = combined_dataset.skip(num_train).take(num_val)\n",
    "new_test_samples = combined_dataset.skip(num_train + num_val)\n",
    "\n",
    "# 나눈 sample에 각각 train, val, test 태그 달기\n",
    "for sample in new_train_samples:\n",
    "    sample.tags = [\"train\"]\n",
    "    sample.save()\n",
    "\n",
    "for sample in new_val_samples:\n",
    "    sample.tags = [\"val\"]\n",
    "    sample.save()\n",
    "\n",
    "for sample in new_test_samples:\n",
    "    sample.tags = [\"test\"]\n",
    "    sample.save()\n",
    "\n",
    "# classes 지정\n",
    "classes = ['cat_toilet']\n",
    "\n",
    "# 새로운 train, val, test 데이터셋 export\n",
    "new_train_samples.export(\n",
    "    export_dir = f\"D:/Data/roboflow/631_dataset/6_cat_toilet/train\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")\n",
    "\n",
    "new_val_samples.export(\n",
    "    export_dir = f\"D:/Data/roboflow/631_dataset/6_cat_toilet/val\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")\n",
    "\n",
    "new_test_samples.export(\n",
    "    export_dir = f\"D:/Data/roboflow/631_dataset/6_cat_toilet/test\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) 7_house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 3710/3710 [2.9s elapsed, 0s remaining, 1.3K samples/s]      \n",
      " 100% |█████████████████| 465/465 [332.1ms elapsed, 0s remaining, 1.4K samples/s]      \n",
      " 100% |█████████████████| 463/463 [344.4ms elapsed, 0s remaining, 1.4K samples/s]      \n",
      "Name:        original_house_train\n",
      "Media type:  image\n",
      "Num samples: 3710\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:         fiftyone.core.fields.ObjectIdField\n",
      "    filepath:   fiftyone.core.fields.StringField\n",
      "    tags:       fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    detections: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "Name:        original_house_val\n",
      "Media type:  image\n",
      "Num samples: 465\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:         fiftyone.core.fields.ObjectIdField\n",
      "    filepath:   fiftyone.core.fields.StringField\n",
      "    tags:       fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    detections: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "Name:        original_house_test\n",
      "Media type:  image\n",
      "Num samples: 463\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:         fiftyone.core.fields.ObjectIdField\n",
      "    filepath:   fiftyone.core.fields.StringField\n",
      "    tags:       fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    detections: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      " 100% |███████████████| 3710/3710 [3.8s elapsed, 0s remaining, 983.8 samples/s]      \n",
      " 100% |█████████████████| 465/465 [451.6ms elapsed, 0s remaining, 1.0K samples/s]      \n",
      " 100% |█████████████████| 463/463 [446.1ms elapsed, 0s remaining, 1.0K samples/s]      \n",
      " 100% |███████████████| 2782/2782 [44.8s elapsed, 0s remaining, 73.1 samples/s]      \n",
      " 100% |███████████████| 1391/1391 [9.9s elapsed, 0s remaining, 163.0 samples/s]       \n",
      " 100% |█████████████████| 465/465 [1.6s elapsed, 0s remaining, 300.3 samples/s]         \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fiftyone as fo\n",
    "import fiftyone.types as fot\n",
    "\n",
    "#기존 데이터셋 로드\n",
    "original_dataset_dir = f\"D:/Data/roboflow/7_house/\"\n",
    "original_train_dataset_dir = os.path.join(original_dataset_dir, \"train\")\n",
    "original_val_dataset_dir = os.path.join(original_dataset_dir, \"val\")\n",
    "original_test_dataset_dir = os.path.join(original_dataset_dir, \"test\")\n",
    "\n",
    "original_train_dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=original_train_dataset_dir,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    name = \"original_house_train\"\n",
    ")\n",
    "\n",
    "original_val_dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=original_val_dataset_dir,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    name = \"original_house_val\"\n",
    ")\n",
    "\n",
    "original_test_dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=original_test_dataset_dir,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    name = \"original_house_test\"\n",
    ")\n",
    "\n",
    "print(original_train_dataset)\n",
    "print(original_val_dataset)\n",
    "print(original_test_dataset)\n",
    "\n",
    "# 기존 데이터셋 하나로 합쳐서 섞기\n",
    "combined_dataset = fo.Dataset(\"combined_house\")\n",
    "\n",
    "combined_dataset.add_samples(original_train_dataset)\n",
    "combined_dataset.add_samples(original_val_dataset)\n",
    "combined_dataset.add_samples(original_test_dataset)\n",
    "\n",
    "combined_dataset.shuffle(seed=41)\n",
    "\n",
    "# train:val:test = 6:3:1 비율로 나누기\n",
    "num_combined_samples = len(combined_dataset)\n",
    "num_train = int(num_combined_samples * 0.6)\n",
    "num_val = int(num_combined_samples * 0.3)\n",
    "\n",
    "new_train_samples = combined_dataset.take(num_train)\n",
    "new_val_samples = combined_dataset.skip(num_train).take(num_val)\n",
    "new_test_samples = combined_dataset.skip(num_train + num_val)\n",
    "\n",
    "# 나눈 sample에 각각 train, val, test 태그 달기\n",
    "for sample in new_train_samples:\n",
    "    sample.tags = [\"train\"]\n",
    "    sample.save()\n",
    "\n",
    "for sample in new_val_samples:\n",
    "    sample.tags = [\"val\"]\n",
    "    sample.save()\n",
    "\n",
    "for sample in new_test_samples:\n",
    "    sample.tags = [\"test\"]\n",
    "    sample.save()\n",
    "\n",
    "# classes 지정\n",
    "classes = ['house']\n",
    "\n",
    "# 새로운 train, val, test 데이터셋 export\n",
    "new_train_samples.export(\n",
    "    export_dir = f\"D:/Data/roboflow/631_dataset/7_house/train\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")\n",
    "\n",
    "new_val_samples.export(\n",
    "    export_dir = f\"D:/Data/roboflow/631_dataset/7_house/val\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")\n",
    "\n",
    "new_test_samples.export(\n",
    "    export_dir = f\"D:/Data/roboflow/631_dataset/7_house/test\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) 8_carrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 2380/2380 [1.8s elapsed, 0s remaining, 1.3K samples/s]         \n",
      " 100% |█████████████████| 298/298 [222.0ms elapsed, 0s remaining, 1.3K samples/s]     \n",
      " 100% |█████████████████| 297/297 [225.8ms elapsed, 0s remaining, 1.3K samples/s]     \n",
      "Name:        original_carrier_train\n",
      "Media type:  image\n",
      "Num samples: 2380\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:         fiftyone.core.fields.ObjectIdField\n",
      "    filepath:   fiftyone.core.fields.StringField\n",
      "    tags:       fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    detections: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "Name:        original_carrier_val\n",
      "Media type:  image\n",
      "Num samples: 298\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:         fiftyone.core.fields.ObjectIdField\n",
      "    filepath:   fiftyone.core.fields.StringField\n",
      "    tags:       fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    detections: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "Name:        original_carrier_test\n",
      "Media type:  image\n",
      "Num samples: 297\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:         fiftyone.core.fields.ObjectIdField\n",
      "    filepath:   fiftyone.core.fields.StringField\n",
      "    tags:       fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    detections: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      " 100% |███████████████| 2380/2380 [2.5s elapsed, 0s remaining, 1.0K samples/s]       \n",
      " 100% |█████████████████| 298/298 [290.2ms elapsed, 0s remaining, 1.0K samples/s]       \n",
      " 100% |█████████████████| 297/297 [295.8ms elapsed, 0s remaining, 1.0K samples/s]      \n",
      " 100% |███████████████| 1785/1785 [25.7s elapsed, 0s remaining, 68.3 samples/s]      \n",
      " 100% |█████████████████| 892/892 [5.2s elapsed, 0s remaining, 145.3 samples/s]      \n",
      " 100% |█████████████████| 298/298 [934.8ms elapsed, 0s remaining, 318.8 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fiftyone as fo\n",
    "import fiftyone.types as fot\n",
    "\n",
    "#기존 데이터셋 로드\n",
    "original_dataset_dir = f\"D:/Data/roboflow/8_carrier/\"\n",
    "original_train_dataset_dir = os.path.join(original_dataset_dir, \"train\")\n",
    "original_val_dataset_dir = os.path.join(original_dataset_dir, \"val\")\n",
    "original_test_dataset_dir = os.path.join(original_dataset_dir, \"test\")\n",
    "\n",
    "original_train_dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=original_train_dataset_dir,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    name = \"original_carrier_train\"\n",
    ")\n",
    "\n",
    "original_val_dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=original_val_dataset_dir,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    name = \"original_carrier_val\"\n",
    ")\n",
    "\n",
    "original_test_dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=original_test_dataset_dir,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    name = \"original_carrier_test\"\n",
    ")\n",
    "\n",
    "print(original_train_dataset)\n",
    "print(original_val_dataset)\n",
    "print(original_test_dataset)\n",
    "\n",
    "# 기존 데이터셋 하나로 합쳐서 섞기\n",
    "combined_dataset = fo.Dataset(\"combined_carrier\")\n",
    "\n",
    "combined_dataset.add_samples(original_train_dataset)\n",
    "combined_dataset.add_samples(original_val_dataset)\n",
    "combined_dataset.add_samples(original_test_dataset)\n",
    "\n",
    "combined_dataset.shuffle(seed=41)\n",
    "\n",
    "# train:val:test = 6:3:1 비율로 나누기\n",
    "num_combined_samples = len(combined_dataset)\n",
    "num_train = int(num_combined_samples * 0.6)\n",
    "num_val = int(num_combined_samples * 0.3)\n",
    "\n",
    "new_train_samples = combined_dataset.take(num_train)\n",
    "new_val_samples = combined_dataset.skip(num_train).take(num_val)\n",
    "new_test_samples = combined_dataset.skip(num_train + num_val)\n",
    "\n",
    "# 나눈 sample에 각각 train, val, test 태그 달기\n",
    "for sample in new_train_samples:\n",
    "    sample.tags = [\"train\"]\n",
    "    sample.save()\n",
    "\n",
    "for sample in new_val_samples:\n",
    "    sample.tags = [\"val\"]\n",
    "    sample.save()\n",
    "\n",
    "for sample in new_test_samples:\n",
    "    sample.tags = [\"test\"]\n",
    "    sample.save()\n",
    "\n",
    "# classes 지정\n",
    "classes = ['carrier']\n",
    "\n",
    "# 새로운 train, val, test 데이터셋 export\n",
    "new_train_samples.export(\n",
    "    export_dir = f\"D:/Data/roboflow/631_dataset/8_carrier/train\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")\n",
    "\n",
    "new_val_samples.export(\n",
    "    export_dir = f\"D:/Data/roboflow/631_dataset/8_carrier/val\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")\n",
    "\n",
    "new_test_samples.export(\n",
    "    export_dir = f\"D:/Data/roboflow/631_dataset/8_carrier/test\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) 9_clothes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 2687/2687 [2.3s elapsed, 0s remaining, 1.1K samples/s]      \n",
      " 100% |█████████████████| 358/358 [294.7ms elapsed, 0s remaining, 1.2K samples/s]     \n",
      " 100% |█████████████████| 342/342 [295.6ms elapsed, 0s remaining, 1.2K samples/s]     \n",
      "Name:        original_clothes_train\n",
      "Media type:  image\n",
      "Num samples: 2687\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:         fiftyone.core.fields.ObjectIdField\n",
      "    filepath:   fiftyone.core.fields.StringField\n",
      "    tags:       fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    detections: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "Name:        original_clothes_val\n",
      "Media type:  image\n",
      "Num samples: 358\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:         fiftyone.core.fields.ObjectIdField\n",
      "    filepath:   fiftyone.core.fields.StringField\n",
      "    tags:       fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    detections: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      "Name:        original_clothes_test\n",
      "Media type:  image\n",
      "Num samples: 342\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:         fiftyone.core.fields.ObjectIdField\n",
      "    filepath:   fiftyone.core.fields.StringField\n",
      "    tags:       fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    detections: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      " 100% |███████████████| 2687/2687 [3.3s elapsed, 0s remaining, 830.7 samples/s]      \n",
      " 100% |█████████████████| 358/358 [394.0ms elapsed, 0s remaining, 912.8 samples/s]      \n",
      " 100% |█████████████████| 342/342 [382.0ms elapsed, 0s remaining, 899.0 samples/s]      \n",
      " 100% |███████████████| 2032/2032 [29.7s elapsed, 0s remaining, 84.3 samples/s]      \n",
      " 100% |███████████████| 1016/1016 [5.8s elapsed, 0s remaining, 205.4 samples/s]      \n",
      " 100% |█████████████████| 339/339 [1.0s elapsed, 0s remaining, 326.3 samples/s]         \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fiftyone as fo\n",
    "import fiftyone.types as fot\n",
    "\n",
    "#기존 데이터셋 로드\n",
    "original_dataset_dir = f\"D:/Data/roboflow/9_clothes/\"\n",
    "original_train_dataset_dir = os.path.join(original_dataset_dir, \"train\")\n",
    "original_val_dataset_dir = os.path.join(original_dataset_dir, \"val\")\n",
    "original_test_dataset_dir = os.path.join(original_dataset_dir, \"test\")\n",
    "\n",
    "original_train_dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=original_train_dataset_dir,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    name = \"original_clothes_train\"\n",
    ")\n",
    "\n",
    "original_val_dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=original_val_dataset_dir,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    name = \"original_clothes_val\"\n",
    ")\n",
    "\n",
    "original_test_dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=original_test_dataset_dir,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    name = \"original_clothes_test\"\n",
    ")\n",
    "\n",
    "print(original_train_dataset)\n",
    "print(original_val_dataset)\n",
    "print(original_test_dataset)\n",
    "\n",
    "# 기존 데이터셋 하나로 합쳐서 섞기\n",
    "combined_dataset = fo.Dataset(\"combined_clothes\")\n",
    "\n",
    "combined_dataset.add_samples(original_train_dataset)\n",
    "combined_dataset.add_samples(original_val_dataset)\n",
    "combined_dataset.add_samples(original_test_dataset)\n",
    "\n",
    "combined_dataset.shuffle(seed=41)\n",
    "\n",
    "# train:val:test = 6:3:1 비율로 나누기\n",
    "num_combined_samples = len(combined_dataset)\n",
    "num_train = int(num_combined_samples * 0.6)\n",
    "num_val = int(num_combined_samples * 0.3)\n",
    "\n",
    "new_train_samples = combined_dataset.take(num_train)\n",
    "new_val_samples = combined_dataset.skip(num_train).take(num_val)\n",
    "new_test_samples = combined_dataset.skip(num_train + num_val)\n",
    "\n",
    "# 나눈 sample에 각각 train, val, test 태그 달기\n",
    "for sample in new_train_samples:\n",
    "    sample.tags = [\"train\"]\n",
    "    sample.save()\n",
    "\n",
    "for sample in new_val_samples:\n",
    "    sample.tags = [\"val\"]\n",
    "    sample.save()\n",
    "\n",
    "for sample in new_test_samples:\n",
    "    sample.tags = [\"test\"]\n",
    "    sample.save()\n",
    "\n",
    "# classes 지정\n",
    "classes = ['clothes']\n",
    "\n",
    "# 새로운 train, val, test 데이터셋 export\n",
    "new_train_samples.export(\n",
    "    export_dir = f\"D:/Data/roboflow/631_dataset/9_clothes/train\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")\n",
    "\n",
    "new_val_samples.export(\n",
    "    export_dir = f\"D:/Data/roboflow/631_dataset/9_clothes/val\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")\n",
    "\n",
    "new_test_samples.export(\n",
    "    export_dir = f\"D:/Data/roboflow/631_dataset/9_clothes/test\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon 데이터 6:3:1 로 분배"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 2_dog_fence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 1928/1928 [1.8s elapsed, 0s remaining, 1.1K samples/s]         \n",
      "Name:        amazon_dogfence_train\n",
      "Media type:  image\n",
      "Num samples: 1928\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:         fiftyone.core.fields.ObjectIdField\n",
      "    filepath:   fiftyone.core.fields.StringField\n",
      "    tags:       fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    detections: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      " 100% |███████████████| 1156/1156 [23.6s elapsed, 0s remaining, 55.2 samples/s]      \n",
      " 100% |█████████████████| 578/578 [6.5s elapsed, 0s remaining, 81.2 samples/s]       \n",
      " 100% |█████████████████| 194/194 [1.3s elapsed, 0s remaining, 144.2 samples/s]         \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fiftyone as fo\n",
    "import fiftyone.types as fot\n",
    "\n",
    "#기존 데이터셋 로드\n",
    "original_dataset_dir = f\"D:/Data/final/Amazon/2_dog_fence/\"\n",
    "original_train_dataset_dir = os.path.join(original_dataset_dir, \"train\")\n",
    "\n",
    "name = \"amazon_dogfence_train\"\n",
    "original_train_dataset = fo.Dataset(name)\n",
    "original_train_dataset.add_dir(\n",
    "    dataset_dir = original_train_dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset\n",
    ")\n",
    "\n",
    "print(original_train_dataset)\n",
    "\n",
    "# train:val:test = 6:3:1 비율로 나누기\n",
    "num_original_samples = len(original_train_dataset)\n",
    "num_train = int(num_original_samples * 0.6)\n",
    "num_val = int(num_original_samples * 0.3)\n",
    "\n",
    "new_train_samples = original_train_dataset.take(num_train)\n",
    "new_val_samples = original_train_dataset.skip(num_train).take(num_val)\n",
    "new_test_samples = original_train_dataset.skip(num_train + num_val)\n",
    "\n",
    "# 나눈 sample에 각각 train, val, test 태그 달기\n",
    "for sample in new_train_samples:\n",
    "    sample.tags = [\"train\"]\n",
    "    sample.save()\n",
    "\n",
    "for sample in new_val_samples:\n",
    "    sample.tags = [\"val\"]\n",
    "    sample.save()\n",
    "\n",
    "for sample in new_test_samples:\n",
    "    sample.tags = [\"test\"]\n",
    "    sample.save()\n",
    "\n",
    "# classes 지정\n",
    "cls = ['dog_fence']\n",
    "\n",
    "# 새로운 train, val, test 데이터셋 export\n",
    "new_train_samples.export(\n",
    "    export_dir = f\"D:/Data/Amazon/631_dataset/2_dog_fence/train\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = cls\n",
    ")\n",
    "\n",
    "new_val_samples.export(\n",
    "    export_dir = f\"D:/Data/Amazon/631_dataset/2_dog_fence/val\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = cls\n",
    ")\n",
    "\n",
    "new_test_samples.export(\n",
    "    export_dir = f\"D:/Data/Amazon/631_dataset/2_dog_fence/test\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = cls\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 3_dog_nosework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 934/934 [1.2s elapsed, 0s remaining, 798.6 samples/s]         \n",
      "Name:        amazon_dognosework_train\n",
      "Media type:  image\n",
      "Num samples: 934\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:         fiftyone.core.fields.ObjectIdField\n",
      "    filepath:   fiftyone.core.fields.StringField\n",
      "    tags:       fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    detections: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      " 100% |█████████████████| 560/560 [9.9s elapsed, 0s remaining, 54.0 samples/s]       \n",
      " 100% |█████████████████| 280/280 [2.9s elapsed, 0s remaining, 102.4 samples/s]      \n",
      " 100% |███████████████████| 94/94 [902.5ms elapsed, 0s remaining, 104.2 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fiftyone as fo\n",
    "import fiftyone.types as fot\n",
    "\n",
    "#기존 데이터셋 로드\n",
    "original_dataset_dir = f\"D:/Data/Amazon/3_dog_nosework/\"\n",
    "original_train_dataset_dir = os.path.join(original_dataset_dir, \"train\")\n",
    "\n",
    "name = \"amazon_dognosework_train\"\n",
    "original_train_dataset = fo.Dataset(name)\n",
    "original_train_dataset.add_dir(\n",
    "    dataset_dir = original_train_dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset\n",
    ")\n",
    "\n",
    "print(original_train_dataset)\n",
    "\n",
    "# train:val:test = 6:3:1 비율로 나누기\n",
    "num_original_samples = len(original_train_dataset)\n",
    "num_train = int(num_original_samples * 0.6)\n",
    "num_val = int(num_original_samples * 0.3)\n",
    "\n",
    "new_train_samples = original_train_dataset.take(num_train)\n",
    "new_val_samples = original_train_dataset.skip(num_train).take(num_val)\n",
    "new_test_samples = original_train_dataset.skip(num_train + num_val)\n",
    "\n",
    "# 나눈 sample에 각각 train, val, test 태그 달기\n",
    "for sample in new_train_samples:\n",
    "    sample.tags = [\"train\"]\n",
    "    sample.save()\n",
    "\n",
    "for sample in new_val_samples:\n",
    "    sample.tags = [\"val\"]\n",
    "    sample.save()\n",
    "\n",
    "for sample in new_test_samples:\n",
    "    sample.tags = [\"test\"]\n",
    "    sample.save()\n",
    "\n",
    "# classes 지정\n",
    "classes = ['dog_nosework']\n",
    "\n",
    "# 새로운 train, val, test 데이터셋 export\n",
    "new_train_samples.export(\n",
    "    export_dir = f\"D:/Data/Amazon/631_dataset/3_dog_nosework/train\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")\n",
    "\n",
    "new_val_samples.export(\n",
    "    export_dir = f\"D:/Data/Amazon/631_dataset/3_dog_nosework/val\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")\n",
    "\n",
    "new_test_samples.export(\n",
    "    export_dir = f\"D:/Data/Amazon/631_dataset/3_dog_nosework/test\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 4_cat_tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 2888/2888 [2.7s elapsed, 0s remaining, 1.1K samples/s]      \n",
      "Name:        amazon_cattower_train\n",
      "Media type:  image\n",
      "Num samples: 2888\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:         fiftyone.core.fields.ObjectIdField\n",
      "    filepath:   fiftyone.core.fields.StringField\n",
      "    tags:       fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    detections: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      " 100% |███████████████| 1732/1732 [32.4s elapsed, 0s remaining, 64.6 samples/s]      \n",
      " 100% |█████████████████| 866/866 [9.3s elapsed, 0s remaining, 127.8 samples/s]      \n",
      " 100% |█████████████████| 290/290 [1.0s elapsed, 0s remaining, 279.9 samples/s]         \n"
     ]
    }
   ],
   "source": [
    "#기존 데이터셋 로드\n",
    "original_dataset_dir = f\"D:/Data/Amazon/4_cat_tower/\"\n",
    "original_train_dataset_dir = os.path.join(original_dataset_dir, \"train\")\n",
    "\n",
    "name = \"amazon_cattower_train\"\n",
    "original_train_dataset = fo.Dataset(name)\n",
    "original_train_dataset.add_dir(\n",
    "    dataset_dir = original_train_dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset\n",
    ")\n",
    "\n",
    "print(original_train_dataset)\n",
    "\n",
    "# train:val:test = 6:3:1 비율로 나누기\n",
    "num_original_samples = len(original_train_dataset)\n",
    "num_train = int(num_original_samples * 0.6)\n",
    "num_val = int(num_original_samples * 0.3)\n",
    "\n",
    "new_train_samples = original_train_dataset.take(num_train)\n",
    "new_val_samples = original_train_dataset.skip(num_train).take(num_val)\n",
    "new_test_samples = original_train_dataset.skip(num_train + num_val)\n",
    "\n",
    "# 나눈 sample에 각각 train, val, test 태그 달기\n",
    "for sample in new_train_samples:\n",
    "    sample.tags = [\"train\"]\n",
    "    sample.save()\n",
    "\n",
    "for sample in new_val_samples:\n",
    "    sample.tags = [\"val\"]\n",
    "    sample.save()\n",
    "\n",
    "for sample in new_test_samples:\n",
    "    sample.tags = [\"test\"]\n",
    "    sample.save()\n",
    "\n",
    "# classes 지정\n",
    "classes = ['cat_tower']\n",
    "\n",
    "# 새로운 train, val, test 데이터셋 export\n",
    "new_train_samples.export(\n",
    "    export_dir = f\"D:/Data/Amazon/631_dataset/4_cat_tower/train\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")\n",
    "\n",
    "new_val_samples.export(\n",
    "    export_dir = f\"D:/Data/Amazon/631_dataset/4_cat_tower/val\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")\n",
    "\n",
    "new_test_samples.export(\n",
    "    export_dir = f\"D:/Data/Amazon/631_dataset/4_cat_tower/test\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) 5_cat_scratcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 1905/1905 [1.5s elapsed, 0s remaining, 1.3K samples/s]         \n",
      "Name:        amazon_catscratcher_train\n",
      "Media type:  image\n",
      "Num samples: 1905\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:         fiftyone.core.fields.ObjectIdField\n",
      "    filepath:   fiftyone.core.fields.StringField\n",
      "    tags:       fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    detections: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      " 100% |███████████████| 1143/1143 [18.0s elapsed, 0s remaining, 64.9 samples/s]      \n",
      " 100% |█████████████████| 571/571 [5.1s elapsed, 0s remaining, 80.5 samples/s]       \n",
      " 100% |█████████████████| 191/191 [748.1ms elapsed, 0s remaining, 255.3 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "#기존 데이터셋 로드\n",
    "original_dataset_dir = f\"D:/Data/Amazon/5_cat_scratcher\"\n",
    "original_train_dataset_dir = os.path.join(original_dataset_dir, \"train\")\n",
    "\n",
    "name = \"amazon_catscratcher_train\"\n",
    "original_train_dataset = fo.Dataset(name)\n",
    "original_train_dataset.add_dir(\n",
    "    dataset_dir = original_train_dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset\n",
    ")\n",
    "\n",
    "print(original_train_dataset)\n",
    "\n",
    "# train:val:test = 6:3:1 비율로 나누기\n",
    "num_original_samples = len(original_train_dataset)\n",
    "num_train = int(num_original_samples * 0.6)\n",
    "num_val = int(num_original_samples * 0.3)\n",
    "\n",
    "new_train_samples = original_train_dataset.take(num_train)\n",
    "new_val_samples = original_train_dataset.skip(num_train).take(num_val)\n",
    "new_test_samples = original_train_dataset.skip(num_train + num_val)\n",
    "\n",
    "# 나눈 sample에 각각 train, val, test 태그 달기\n",
    "for sample in new_train_samples:\n",
    "    sample.tags = [\"train\"]\n",
    "    sample.save()\n",
    "\n",
    "for sample in new_val_samples:\n",
    "    sample.tags = [\"val\"]\n",
    "    sample.save()\n",
    "\n",
    "for sample in new_test_samples:\n",
    "    sample.tags = [\"test\"]\n",
    "    sample.save()\n",
    "\n",
    "# classes 지정\n",
    "classes = ['cat_scratcher']\n",
    "\n",
    "# 새로운 train, val, test 데이터셋 export\n",
    "new_train_samples.export(\n",
    "    export_dir = f\"D:/Data/Amazon/631_dataset/5_cat_scratcher/train\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")\n",
    "\n",
    "new_val_samples.export(\n",
    "    export_dir = f\"D:/Data/Amazon/631_dataset/5_cat_scratcher/val\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")\n",
    "\n",
    "new_test_samples.export(\n",
    "    export_dir = f\"D:/Data/Amazon/631_dataset/5_cat_scratcher/test\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) 6_cat_toilet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 1800/1800 [1.8s elapsed, 0s remaining, 1.0K samples/s]       \n",
      "Name:        amazon_cattoilet_train\n",
      "Media type:  image\n",
      "Num samples: 1800\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:         fiftyone.core.fields.ObjectIdField\n",
      "    filepath:   fiftyone.core.fields.StringField\n",
      "    tags:       fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    detections: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      " 100% |███████████████| 1080/1080 [15.7s elapsed, 0s remaining, 94.2 samples/s]      \n",
      " 100% |█████████████████| 540/540 [4.9s elapsed, 0s remaining, 58.7 samples/s]       \n",
      " 100% |█████████████████| 180/180 [2.1s elapsed, 0s remaining, 86.3 samples/s]       \n"
     ]
    }
   ],
   "source": [
    "#기존 데이터셋 로드\n",
    "original_dataset_dir = f\"D:/Data/Amazon/6_cat_toilet/\"\n",
    "original_train_dataset_dir = os.path.join(original_dataset_dir, \"train\")\n",
    "\n",
    "name = \"amazon_cattoilet_train\"\n",
    "original_train_dataset = fo.Dataset(name)\n",
    "original_train_dataset.add_dir(\n",
    "    dataset_dir = original_train_dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset\n",
    ")\n",
    "\n",
    "print(original_train_dataset)\n",
    "\n",
    "# train:val:test = 6:3:1 비율로 나누기\n",
    "num_original_samples = len(original_train_dataset)\n",
    "num_train = int(num_original_samples * 0.6)\n",
    "num_val = int(num_original_samples * 0.3)\n",
    "\n",
    "new_train_samples = original_train_dataset.take(num_train)\n",
    "new_val_samples = original_train_dataset.skip(num_train).take(num_val)\n",
    "new_test_samples = original_train_dataset.skip(num_train + num_val)\n",
    "\n",
    "# 나눈 sample에 각각 train, val, test 태그 달기\n",
    "for sample in new_train_samples:\n",
    "    sample.tags = [\"train\"]\n",
    "    sample.save()\n",
    "\n",
    "for sample in new_val_samples:\n",
    "    sample.tags = [\"val\"]\n",
    "    sample.save()\n",
    "\n",
    "for sample in new_test_samples:\n",
    "    sample.tags = [\"test\"]\n",
    "    sample.save()\n",
    "\n",
    "# classes 지정\n",
    "classes = ['cat_toilet']\n",
    "\n",
    "# 새로운 train, val, test 데이터셋 export\n",
    "new_train_samples.export(\n",
    "    export_dir = f\"D:/Data/Amazon/631_dataset/6_cat_toilet/train\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")\n",
    "\n",
    "new_val_samples.export(\n",
    "    export_dir = f\"D:/Data/Amazon/631_dataset/6_cat_toilet/val\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")\n",
    "\n",
    "new_test_samples.export(\n",
    "    export_dir = f\"D:/Data/Amazon/631_dataset/6_cat_toilet/test\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) 7_house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 데이터 수 충분해서 추가 크롤링 진행 X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) 8_carrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 926/926 [966.3ms elapsed, 0s remaining, 958.3 samples/s]      \n",
      "Name:        amazon_carrier_train\n",
      "Media type:  image\n",
      "Num samples: 926\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:         fiftyone.core.fields.ObjectIdField\n",
      "    filepath:   fiftyone.core.fields.StringField\n",
      "    tags:       fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    detections: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      " 100% |█████████████████| 555/555 [7.5s elapsed, 0s remaining, 58.2 samples/s]       \n",
      " 100% |█████████████████| 277/277 [1.6s elapsed, 0s remaining, 179.7 samples/s]         \n",
      " 100% |███████████████████| 94/94 [352.0ms elapsed, 0s remaining, 267.1 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "#기존 데이터셋 로드\n",
    "original_dataset_dir = f\"D:/Data/Amazon/8_carrier/\"\n",
    "original_train_dataset_dir = os.path.join(original_dataset_dir, \"train\")\n",
    "\n",
    "name = \"amazon_carrier_train\"\n",
    "original_train_dataset = fo.Dataset(name)\n",
    "original_train_dataset.add_dir(\n",
    "    dataset_dir = original_train_dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset\n",
    ")\n",
    "\n",
    "print(original_train_dataset)\n",
    "\n",
    "# train:val:test = 6:3:1 비율로 나누기\n",
    "num_original_samples = len(original_train_dataset)\n",
    "num_train = int(num_original_samples * 0.6)\n",
    "num_val = int(num_original_samples * 0.3)\n",
    "\n",
    "new_train_samples = original_train_dataset.take(num_train)\n",
    "new_val_samples = original_train_dataset.skip(num_train).take(num_val)\n",
    "new_test_samples = original_train_dataset.skip(num_train + num_val)\n",
    "\n",
    "# 나눈 sample에 각각 train, val, test 태그 달기\n",
    "for sample in new_train_samples:\n",
    "    sample.tags = [\"train\"]\n",
    "    sample.save()\n",
    "\n",
    "for sample in new_val_samples:\n",
    "    sample.tags = [\"val\"]\n",
    "    sample.save()\n",
    "\n",
    "for sample in new_test_samples:\n",
    "    sample.tags = [\"test\"]\n",
    "    sample.save()\n",
    "\n",
    "# classes 지정\n",
    "classes = ['carrier']\n",
    "\n",
    "# 새로운 train, val, test 데이터셋 export\n",
    "new_train_samples.export(\n",
    "    export_dir = f\"D:/Data/Amazon/631_dataset/8_carrier/train\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")\n",
    "\n",
    "new_val_samples.export(\n",
    "    export_dir = f\"D:/Data/Amazon/631_dataset/8_carrier/val\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")\n",
    "\n",
    "new_test_samples.export(\n",
    "    export_dir = f\"D:/Data/Amazon/631_dataset/8_carrier/test\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) 9_clothes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 100/100 [124.5ms elapsed, 0s remaining, 815.3 samples/s] \n",
      "Name:        amazon_clothes_train\n",
      "Media type:  image\n",
      "Num samples: 100\n",
      "Persistent:  False\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:         fiftyone.core.fields.ObjectIdField\n",
      "    filepath:   fiftyone.core.fields.StringField\n",
      "    tags:       fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    detections: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
      " 100% |███████████████████| 60/60 [917.4ms elapsed, 0s remaining, 65.4 samples/s]      \n",
      " 100% |███████████████████| 30/30 [234.7ms elapsed, 0s remaining, 127.8 samples/s]     \n",
      " 100% |███████████████████| 10/10 [50.9ms elapsed, 0s remaining, 196.4 samples/s] \n"
     ]
    }
   ],
   "source": [
    "#기존 데이터셋 로드\n",
    "original_dataset_dir = f\"D:/Data/final/Amazon/9_clothes/\"\n",
    "original_train_dataset_dir = os.path.join(original_dataset_dir, \"train\")\n",
    "\n",
    "name = \"amazon_clothes_train\"\n",
    "original_train_dataset = fo.Dataset(name)\n",
    "original_train_dataset.add_dir(\n",
    "    dataset_dir = original_train_dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset\n",
    ")\n",
    "\n",
    "print(original_train_dataset)\n",
    "\n",
    "# train:val:test = 6:3:1 비율로 나누기\n",
    "num_original_samples = len(original_train_dataset)\n",
    "num_train = int(num_original_samples * 0.6)\n",
    "num_val = int(num_original_samples * 0.3)\n",
    "\n",
    "new_train_samples = original_train_dataset.take(num_train)\n",
    "new_val_samples = original_train_dataset.skip(num_train).take(num_val)\n",
    "new_test_samples = original_train_dataset.skip(num_train + num_val)\n",
    "\n",
    "# 나눈 sample에 각각 train, val, test 태그 달기\n",
    "for sample in new_train_samples:\n",
    "    sample.tags = [\"train\"]\n",
    "    sample.save()\n",
    "\n",
    "for sample in new_val_samples:\n",
    "    sample.tags = [\"val\"]\n",
    "    sample.save()\n",
    "\n",
    "for sample in new_test_samples:\n",
    "    sample.tags = [\"test\"]\n",
    "    sample.save()\n",
    "\n",
    "# classes 지정\n",
    "classes = ['clothes']\n",
    "\n",
    "# 새로운 train, val, test 데이터셋 export\n",
    "new_train_samples.export(\n",
    "    export_dir = f\"D:/Data/final/Amazon/631_dataset/9_clothes/train\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")\n",
    "\n",
    "new_val_samples.export(\n",
    "    export_dir = f\"D:/Data/final/Amazon/631_dataset/9_clothes/val\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")\n",
    "\n",
    "new_test_samples.export(\n",
    "    export_dir = f\"D:/Data/final/Amazon/631_dataset/9_clothes/test\",\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    "    classes = classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coupang + Amazon 합치기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 각 class 별 데이터 합치기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2_dog_fence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 760/760 [696.1ms elapsed, 0s remaining, 1.1K samples/s]      \n",
      " 100% |███████████████| 1156/1156 [1.1s elapsed, 0s remaining, 1.1K samples/s]         \n",
      " 100% |███████████████| 1916/1916 [19.7s elapsed, 0s remaining, 134.6 samples/s]      \n",
      " 100% |█████████████████| 380/380 [394.0ms elapsed, 0s remaining, 969.1 samples/s]      \n",
      " 100% |█████████████████| 578/578 [532.3ms elapsed, 0s remaining, 1.1K samples/s]      \n",
      " 100% |█████████████████| 958/958 [9.8s elapsed, 0s remaining, 133.9 samples/s]      \n",
      " 100% |█████████████████| 128/128 [147.3ms elapsed, 0s remaining, 868.9 samples/s]    \n",
      " 100% |█████████████████| 194/194 [174.3ms elapsed, 0s remaining, 1.1K samples/s]     \n",
      " 100% |█████████████████| 322/322 [3.8s elapsed, 0s remaining, 140.3 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "merge_classes = [\"dog_fence\", \"dog_nosework\", \"cat_tower\", \"cat_scratcher\", \"cat_toilet\", \"carrier\", \"clothes\"]\n",
    "\n",
    "# Train\n",
    "name_train = \"dogfence_merge_train\"\n",
    "dataset_train = fo.Dataset(name_train)\n",
    "\n",
    "    # Coupang\n",
    "dataset_dir = f\"D:/Data/final/Coupang/631_dataset/dog_fence/train\"\n",
    "dataset_train.add_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "    # Amazon\n",
    "dataset_dir2 = f\"D:/Data/final/Amazon/631_dataset/dog_fence/train\"\n",
    "dataset_train.add_dir(\n",
    "    dataset_dir = dataset_dir2,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "    # Export\n",
    "export_dir = f\"D:/Data/final/merged/dog_fence/train\"\n",
    "\n",
    "dataset_train.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    "    classes=[\"dog_fence\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Val\n",
    "name_val = \"dog_fence_merge_val\"\n",
    "dataset_val = fo.Dataset(name_val)\n",
    "\n",
    "    # Coupang\n",
    "dataset_dir = f\"D:/Data/final/Coupang/631_dataset/dog_fence/val\"\n",
    "dataset_val.add_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "    # Amazon\n",
    "dataset_dir2 = f\"D:/Data/final/Amazon/631_dataset/dog_fence/val\"\n",
    "dataset_val.add_dir(\n",
    "    dataset_dir = dataset_dir2,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "    # Export\n",
    "export_dir = f\"D:/Data/final/merged/dog_fence/val\"\n",
    "\n",
    "dataset_val.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    "    classes=[\"dog_fence\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Test\n",
    "name_test = \"dog_fence_merge_test\"\n",
    "dataset_test = fo.Dataset(name_test)\n",
    "\n",
    "    # Coupang\n",
    "dataset_dir = f\"D:/Data/final/Coupang/631_dataset/dog_fence/test\"\n",
    "dataset_test.add_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "    # Amazon\n",
    "dataset_dir2 = f\"D:/Data/final/Amazon/631_dataset/dog_fence/test\"\n",
    "dataset_test.add_dir(\n",
    "    dataset_dir = dataset_dir2,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "    # Export\n",
    "export_dir = f\"D:/Data/final/merged/dog_fence/test\"\n",
    "\n",
    "dataset_test.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    "    classes=[\"dog_fence\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3_dog_nosework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 791/791 [876.3ms elapsed, 0s remaining, 902.7 samples/s]      \n",
      " 100% |█████████████████| 560/560 [531.4ms elapsed, 0s remaining, 1.1K samples/s]      \n",
      " 100% |███████████████| 1351/1351 [21.5s elapsed, 0s remaining, 57.9 samples/s]      \n",
      " 100% |█████████████████| 395/395 [413.8ms elapsed, 0s remaining, 958.8 samples/s]      \n",
      " 100% |█████████████████| 280/280 [384.4ms elapsed, 0s remaining, 732.9 samples/s]      \n",
      " 100% |█████████████████| 675/675 [11.3s elapsed, 0s remaining, 42.2 samples/s]      \n",
      " 100% |█████████████████| 133/133 [176.3ms elapsed, 0s remaining, 754.6 samples/s]    \n",
      " 100% |███████████████████| 94/94 [92.4ms elapsed, 0s remaining, 1.0K samples/s]      \n",
      " 100% |█████████████████| 227/227 [3.3s elapsed, 0s remaining, 59.1 samples/s]       \n"
     ]
    }
   ],
   "source": [
    "merge_classes = [\"dog_fence\", \"dog_nosework\", \"cat_tower\", \"cat_scratcher\", \"cat_toilet\", \"carrier\", \"clothes\"]\n",
    "\n",
    "# Train\n",
    "name_train = \"dognosework_merge_train\"\n",
    "dataset_train = fo.Dataset(name_train)\n",
    "\n",
    "    # Coupang\n",
    "dataset_dir = f\"D:/Data/final/Coupang/631_dataset/dog_nosework/train\"\n",
    "dataset_train.add_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "    # Amazon\n",
    "dataset_dir2 = f\"D:/Data/final/Amazon/631_dataset/dog_nosework/train\"\n",
    "dataset_train.add_dir(\n",
    "    dataset_dir = dataset_dir2,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "    # Export\n",
    "export_dir = f\"D:/Data/final/merged/3_dog_nosework/train\"\n",
    "\n",
    "dataset_train.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    "    classes=[\"dog_nosework\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Val\n",
    "name_val = \"dog_nosework_merge_val\"\n",
    "dataset_val = fo.Dataset(name_val)\n",
    "\n",
    "    # Coupang\n",
    "dataset_dir = f\"D:/Data/final/Coupang/631_dataset/dog_nosework/val\"\n",
    "dataset_val.add_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "    # Amazon\n",
    "dataset_dir2 = f\"D:/Data/final/Amazon/631_dataset/dog_nosework/val\"\n",
    "dataset_val.add_dir(\n",
    "    dataset_dir = dataset_dir2,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "    # Export\n",
    "export_dir = f\"D:/Data/final/merged/3_dog_nosework/val\"\n",
    "\n",
    "dataset_val.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    "    classes=[\"dog_nosework\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Test\n",
    "name_test = \"dog_nosework_merge_test\"\n",
    "dataset_test = fo.Dataset(name_test)\n",
    "\n",
    "    # Coupang\n",
    "dataset_dir = f\"D:/Data/final/Coupang/631_dataset/dog_nosework/test\"\n",
    "dataset_test.add_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "    # Amazon\n",
    "dataset_dir2 = f\"D:/Data/final/Amazon/631_dataset/dog_nosework/test\"\n",
    "dataset_test.add_dir(\n",
    "    dataset_dir = dataset_dir2,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "    # Export\n",
    "export_dir = f\"D:/Data/final/merged/3_dog_nosework/test\"\n",
    "\n",
    "dataset_test.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    "    classes=[\"dog_nosework\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4_cat_tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 727/727 [548.9ms elapsed, 0s remaining, 1.3K samples/s]      \n",
      " 100% |███████████████| 1732/1732 [1.5s elapsed, 0s remaining, 1.1K samples/s]         \n",
      " 100% |███████████████| 2459/2459 [33.4s elapsed, 0s remaining, 83.2 samples/s]      \n",
      " 100% |█████████████████| 363/363 [273.2ms elapsed, 0s remaining, 1.3K samples/s]     \n",
      " 100% |█████████████████| 866/866 [767.5ms elapsed, 0s remaining, 1.1K samples/s]      \n",
      " 100% |███████████████| 1229/1229 [14.3s elapsed, 0s remaining, 79.3 samples/s]       \n",
      " 100% |█████████████████| 123/123 [105.0ms elapsed, 0s remaining, 1.2K samples/s]  \n",
      " 100% |█████████████████| 290/290 [230.7ms elapsed, 0s remaining, 1.3K samples/s]     \n",
      " 100% |█████████████████| 413/413 [4.7s elapsed, 0s remaining, 96.8 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "merge_classes = [\"dog_fence\", \"dog_nosework\", \"cat_tower\", \"cat_scratcher\", \"cat_toilet\", \"carrier\", \"clothes\"]\n",
    "\n",
    "# Train\n",
    "name_train = \"cattower_merge_train\"\n",
    "dataset_train = fo.Dataset(name_train)\n",
    "\n",
    "    # Coupang\n",
    "dataset_dir = f\"D:/Data/final/Coupang/631_dataset/cat_tower/train\"\n",
    "dataset_train.add_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "    # Amazon\n",
    "dataset_dir2 = f\"D:/Data/final/Amazon/631_dataset/cat_tower/train\"\n",
    "dataset_train.add_dir(\n",
    "    dataset_dir = dataset_dir2,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "    # Export\n",
    "export_dir = f\"D:/Data/final/merged/4_cat_tower/train\"\n",
    "\n",
    "dataset_train.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    "    classes=[\"cat_tower\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Val\n",
    "name_val = \"cat_tower_merge_val\"\n",
    "dataset_val = fo.Dataset(name_val)\n",
    "\n",
    "    # Coupang\n",
    "dataset_dir = f\"D:/Data/final/Coupang/631_dataset/cat_tower/val\"\n",
    "dataset_val.add_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "    # Amazon\n",
    "dataset_dir2 = f\"D:/Data/final/Amazon/631_dataset/cat_tower/val\"\n",
    "dataset_val.add_dir(\n",
    "    dataset_dir = dataset_dir2,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "    # Export\n",
    "export_dir = f\"D:/Data/final/merged/4_cat_tower/val\"\n",
    "\n",
    "dataset_val.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    "    classes=[\"cat_tower\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Test\n",
    "name_test = \"cat_tower_merge_test\"\n",
    "dataset_test = fo.Dataset(name_test)\n",
    "\n",
    "    # Coupang\n",
    "dataset_dir = f\"D:/Data/final/Coupang/631_dataset/cat_tower/test\"\n",
    "dataset_test.add_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "    # Amazon\n",
    "dataset_dir2 = f\"D:/Data/final/Amazon/631_dataset/cat_tower/test\"\n",
    "dataset_test.add_dir(\n",
    "    dataset_dir = dataset_dir2,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "    # Export\n",
    "export_dir = f\"D:/Data/final/merged/4_cat_tower/test\"\n",
    "\n",
    "dataset_test.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    "    classes=[\"cat_tower\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5_cat_scratcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 1354/1354 [1.2s elapsed, 0s remaining, 1.1K samples/s]         \n",
      " 100% |███████████████| 1143/1143 [713.4ms elapsed, 0s remaining, 1.6K samples/s]      \n",
      " 100% |███████████████| 2497/2497 [27.1s elapsed, 0s remaining, 85.2 samples/s]       \n",
      " 100% |█████████████████| 677/677 [552.1ms elapsed, 0s remaining, 1.2K samples/s]      \n",
      " 100% |█████████████████| 571/571 [365.0ms elapsed, 0s remaining, 1.6K samples/s]      \n",
      " 100% |███████████████| 1248/1248 [12.8s elapsed, 0s remaining, 112.5 samples/s]      \n",
      " 100% |█████████████████| 227/227 [200.1ms elapsed, 0s remaining, 1.1K samples/s]     \n",
      " 100% |█████████████████| 191/191 [126.9ms elapsed, 0s remaining, 1.5K samples/s]     \n",
      " 100% |█████████████████| 418/418 [4.3s elapsed, 0s remaining, 99.4 samples/s]       \n"
     ]
    }
   ],
   "source": [
    "merge_classes = [\"dog_fence\", \"dog_nosework\", \"cat_tower\", \"cat_scratcher\", \"cat_toilet\", \"carrier\", \"clothes\"]\n",
    "\n",
    "# Train\n",
    "name_train = \"catscratcher_merge_train\"\n",
    "dataset_train = fo.Dataset(name_train)\n",
    "\n",
    "    # Coupang\n",
    "dataset_dir = f\"D:/Data/final/Coupang/631_dataset/cat_scratcher/train\"\n",
    "dataset_train.add_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "    # Amazon\n",
    "dataset_dir2 = f\"D:/Data/final/Amazon/631_dataset/cat_scratcher/train\"\n",
    "dataset_train.add_dir(\n",
    "    dataset_dir = dataset_dir2,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "    # Export\n",
    "export_dir = f\"D:/Data/final/merged/5_cat_scratcher/train\"\n",
    "\n",
    "dataset_train.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    "    classes=[\"cat_scratcher\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Val\n",
    "name_val = \"cat_scratcher_merge_val\"\n",
    "dataset_val = fo.Dataset(name_val)\n",
    "\n",
    "    # Coupang\n",
    "dataset_dir = f\"D:/Data/final/Coupang/631_dataset/cat_scratcher/val\"\n",
    "dataset_val.add_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "    # Amazon\n",
    "dataset_dir2 = f\"D:/Data/final/Amazon/631_dataset/cat_scratcher/val\"\n",
    "dataset_val.add_dir(\n",
    "    dataset_dir = dataset_dir2,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "    # Export\n",
    "export_dir = f\"D:/Data/final/merged/5_cat_scratcher/val\"\n",
    "\n",
    "dataset_val.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    "    classes=[\"cat_scratcher\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Test\n",
    "name_test = \"cat_scratcher_merge_test\"\n",
    "dataset_test = fo.Dataset(name_test)\n",
    "\n",
    "    # Coupang\n",
    "dataset_dir = f\"D:/Data/final/Coupang/631_dataset/cat_scratcher/test\"\n",
    "dataset_test.add_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "    # Amazon\n",
    "dataset_dir2 = f\"D:/Data/final/Amazon/631_dataset/cat_scratcher/test\"\n",
    "dataset_test.add_dir(\n",
    "    dataset_dir = dataset_dir2,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "    # Export\n",
    "export_dir = f\"D:/Data/final/merged/5_cat_scratcher/test\"\n",
    "\n",
    "dataset_test.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    "    classes=[\"cat_scratcher\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6_cat_toilet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 1406/1406 [1.2s elapsed, 0s remaining, 1.2K samples/s]         \n",
      " 100% |███████████████| 1080/1080 [1.1s elapsed, 0s remaining, 961.2 samples/s]         \n",
      " 100% |███████████████| 2486/2486 [28.2s elapsed, 0s remaining, 91.2 samples/s]       \n",
      " 100% |█████████████████| 703/703 [698.1ms elapsed, 0s remaining, 1.0K samples/s]       \n",
      " 100% |█████████████████| 540/540 [507.2ms elapsed, 0s remaining, 1.1K samples/s]      \n",
      " 100% |███████████████| 1243/1243 [12.8s elapsed, 0s remaining, 104.2 samples/s]      \n",
      " 100% |█████████████████| 235/235 [206.7ms elapsed, 0s remaining, 1.1K samples/s]      \n",
      " 100% |█████████████████| 180/180 [177.0ms elapsed, 0s remaining, 1.0K samples/s]     \n",
      " 100% |█████████████████| 415/415 [4.3s elapsed, 0s remaining, 95.9 samples/s]       \n"
     ]
    }
   ],
   "source": [
    "merge_classes = [\"dog_fence\", \"dog_nosework\", \"cat_tower\", \"cat_scratcher\", \"cat_toilet\", \"carrier\", \"clothes\"]\n",
    "\n",
    "# Train\n",
    "name_train = \"cattoilet_merge_train\"\n",
    "dataset_train = fo.Dataset(name_train)\n",
    "\n",
    "    # Coupang\n",
    "dataset_dir = f\"D:/Data/final/Coupang/631_dataset/cat_toilet/train\"\n",
    "dataset_train.add_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "    # Amazon\n",
    "dataset_dir2 = f\"D:/Data/final/Amazon/631_dataset/cat_toilet/train\"\n",
    "dataset_train.add_dir(\n",
    "    dataset_dir = dataset_dir2,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "    # Export\n",
    "export_dir = f\"D:/Data/final/merged/6_cat_toilet/train\"\n",
    "\n",
    "dataset_train.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    "    classes=[\"cat_toilet\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Val\n",
    "name_val = \"cat_toilet_merge_val\"\n",
    "dataset_val = fo.Dataset(name_val)\n",
    "\n",
    "    # Coupang\n",
    "dataset_dir = f\"D:/Data/final/Coupang/631_dataset/cat_toilet/val\"\n",
    "dataset_val.add_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "    # Amazon\n",
    "dataset_dir2 = f\"D:/Data/final/Amazon/631_dataset/cat_toilet/val\"\n",
    "dataset_val.add_dir(\n",
    "    dataset_dir = dataset_dir2,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "    # Export\n",
    "export_dir = f\"D:/Data/final/merged/6_cat_toilet/val\"\n",
    "\n",
    "dataset_val.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    "    classes=[\"cat_toilet\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Test\n",
    "name_test = \"cat_toilet_merge_test\"\n",
    "dataset_test = fo.Dataset(name_test)\n",
    "\n",
    "    # Coupang\n",
    "dataset_dir = f\"D:/Data/final/Coupang/631_dataset/cat_toilet/test\"\n",
    "dataset_test.add_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "    # Amazon\n",
    "dataset_dir2 = f\"D:/Data/final/Amazon/631_dataset/cat_toilet/test\"\n",
    "dataset_test.add_dir(\n",
    "    dataset_dir = dataset_dir2,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "    # Export\n",
    "export_dir = f\"D:/Data/final/merged/6_cat_toilet/test\"\n",
    "\n",
    "dataset_test.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    "    classes=[\"cat_toilet\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7_house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 2782/2782 [2.4s elapsed, 0s remaining, 1.1K samples/s]       \n",
      " 100% |███████████████| 2782/2782 [29.5s elapsed, 0s remaining, 102.1 samples/s]      \n",
      " 100% |███████████████| 1391/1391 [1.2s elapsed, 0s remaining, 1.2K samples/s]         \n",
      " 100% |███████████████| 1391/1391 [15.7s elapsed, 0s remaining, 51.1 samples/s]       \n",
      " 100% |█████████████████| 465/465 [514.9ms elapsed, 0s remaining, 907.2 samples/s]      \n",
      " 100% |█████████████████| 465/465 [4.9s elapsed, 0s remaining, 101.9 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "merge_classes = [\"dog_fence\", \"dog_nosework\", \"cat_tower\", \"cat_scratcher\", \"cat_toilet\", \"carrier\", \"clothes\"]\n",
    "\n",
    "# Train\n",
    "name_train = \"house_merge_train\"\n",
    "dataset_train = fo.Dataset(name_train)\n",
    "\n",
    "    # Coupang\n",
    "dataset_dir = f\"D:/Data/final/Coupang/631_dataset/house/train\"\n",
    "dataset_train.add_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "    # Export\n",
    "export_dir = f\"D:/Data/final/merged/7_house/train\"\n",
    "\n",
    "dataset_train.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    "    classes=[\"house\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Val\n",
    "name_val = \"house_merge_val\"\n",
    "dataset_val = fo.Dataset(name_val)\n",
    "\n",
    "    # Coupang\n",
    "dataset_dir = f\"D:/Data/final/Coupang/631_dataset/house/val\"\n",
    "dataset_val.add_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "    # Export\n",
    "export_dir = f\"D:/Data/final/merged/7_house/val\"\n",
    "\n",
    "dataset_val.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    "    classes=[\"house\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Test\n",
    "name_test = \"house_merge_test\"\n",
    "dataset_test = fo.Dataset(name_test)\n",
    "\n",
    "    # Coupang\n",
    "dataset_dir = f\"D:/Data/final/Coupang/631_dataset/house/test\"\n",
    "dataset_test.add_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "    # Export\n",
    "export_dir = f\"D:/Data/final/merged/7_house/test\"\n",
    "\n",
    "dataset_test.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    "    classes=[\"house\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8_carrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 1785/1785 [1.4s elapsed, 0s remaining, 1.3K samples/s]         \n",
      " 100% |█████████████████| 555/555 [487.2ms elapsed, 0s remaining, 1.1K samples/s]      \n",
      " 100% |███████████████| 2340/2340 [23.7s elapsed, 0s remaining, 78.6 samples/s]       \n",
      " 100% |█████████████████| 892/892 [692.2ms elapsed, 0s remaining, 1.3K samples/s]      \n",
      " 100% |█████████████████| 277/277 [245.4ms elapsed, 0s remaining, 1.1K samples/s]      \n",
      " 100% |███████████████| 1169/1169 [7.4s elapsed, 0s remaining, 154.1 samples/s]       \n",
      " 100% |█████████████████| 298/298 [245.6ms elapsed, 0s remaining, 1.2K samples/s]     \n",
      " 100% |███████████████████| 94/94 [91.9ms elapsed, 0s remaining, 1.0K samples/s]      \n",
      " 100% |█████████████████| 392/392 [1.9s elapsed, 0s remaining, 212.4 samples/s]         \n"
     ]
    }
   ],
   "source": [
    "merge_classes = [\"dog_fence\", \"dog_nosework\", \"cat_tower\", \"cat_scratcher\", \"cat_toilet\", \"carrier\", \"clothes\"]\n",
    "\n",
    "# Train\n",
    "name_train = \"carrier_merge_train\"\n",
    "dataset_train = fo.Dataset(name_train)\n",
    "\n",
    "    # Coupang\n",
    "dataset_dir = f\"D:/Data/final/Coupang/631_dataset/carrier/train\"\n",
    "dataset_train.add_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "    # Amazon\n",
    "dataset_dir2 = f\"D:/Data/final/Amazon/631_dataset/carrier/train\"\n",
    "dataset_train.add_dir(\n",
    "    dataset_dir = dataset_dir2,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "    # Export\n",
    "export_dir = f\"D:/Data/final/merged/8_carrier/train\"\n",
    "\n",
    "dataset_train.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    "    classes=[\"carrier\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Val\n",
    "name_val = \"carrier_merge_val\"\n",
    "dataset_val = fo.Dataset(name_val)\n",
    "\n",
    "    # Coupang\n",
    "dataset_dir = f\"D:/Data/final/Coupang/631_dataset/carrier/val\"\n",
    "dataset_val.add_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "    # Amazon\n",
    "dataset_dir2 = f\"D:/Data/final/Amazon/631_dataset/carrier/val\"\n",
    "dataset_val.add_dir(\n",
    "    dataset_dir = dataset_dir2,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "    # Export\n",
    "export_dir = f\"D:/Data/final/merged/8_carrier/val\"\n",
    "\n",
    "dataset_val.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    "    classes=[\"carrier\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Test\n",
    "name_test = \"carrier_merge_test\"\n",
    "dataset_test = fo.Dataset(name_test)\n",
    "\n",
    "    # Coupang\n",
    "dataset_dir = f\"D:/Data/final/Coupang/631_dataset/carrier/test\"\n",
    "dataset_test.add_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "    # Amazon\n",
    "dataset_dir2 = f\"D:/Data/final/Amazon/631_dataset/carrier/test\"\n",
    "dataset_test.add_dir(\n",
    "    dataset_dir = dataset_dir2,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "    # Export\n",
    "export_dir = f\"D:/Data/final/merged/8_carrier/test\"\n",
    "\n",
    "dataset_test.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    "    classes=[\"carrier\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9_clothes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 2032/2032 [2.1s elapsed, 0s remaining, 967.2 samples/s]      \n",
      " 100% |███████████████████| 60/60 [65.6ms elapsed, 0s remaining, 914.8 samples/s]    \n",
      " 100% |███████████████| 2092/2092 [21.9s elapsed, 0s remaining, 74.2 samples/s]       \n",
      " 100% |███████████████| 1016/1016 [883.9ms elapsed, 0s remaining, 1.1K samples/s]       \n",
      " 100% |███████████████████| 30/30 [34.5ms elapsed, 0s remaining, 869.5 samples/s]    \n",
      " 100% |███████████████| 1046/1046 [10.1s elapsed, 0s remaining, 48.8 samples/s]      \n",
      " 100% |█████████████████| 339/339 [311.1ms elapsed, 0s remaining, 1.1K samples/s]      \n",
      " 100% |███████████████████| 10/10 [13.1ms elapsed, 0s remaining, 765.7 samples/s]    \n",
      " 100% |█████████████████| 349/349 [5.9s elapsed, 0s remaining, 64.7 samples/s]       \n"
     ]
    }
   ],
   "source": [
    "merge_classes = [\"dog_fence\", \"dog_nosework\", \"cat_tower\", \"cat_scratcher\", \"cat_toilet\", \"carrier\", \"clothes\"]\n",
    "\n",
    "# Train\n",
    "name_train = \"clothes_merge_train\"\n",
    "dataset_train = fo.Dataset(name_train)\n",
    "\n",
    "    # Coupang\n",
    "dataset_dir = f\"D:/Data/final/Coupang/631_dataset/clothes/train\"\n",
    "dataset_train.add_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "    # Amazon\n",
    "dataset_dir2 = f\"D:/Data/final/Amazon/631_dataset/clothes/train\"\n",
    "dataset_train.add_dir(\n",
    "    dataset_dir = dataset_dir2,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "    # Export\n",
    "export_dir = f\"D:/Data/final/merged/9_clothes/train\"\n",
    "\n",
    "dataset_train.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    "    classes=[\"clothes\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Val\n",
    "name_val = \"clothes_merge_val\"\n",
    "dataset_val = fo.Dataset(name_val)\n",
    "\n",
    "    # Coupang\n",
    "dataset_dir = f\"D:/Data/final/Coupang/631_dataset/clothes/val\"\n",
    "dataset_val.add_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "    # Amazon\n",
    "dataset_dir2 = f\"D:/Data/final/Amazon/631_dataset/clothes/val\"\n",
    "dataset_val.add_dir(\n",
    "    dataset_dir = dataset_dir2,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "    # Export\n",
    "export_dir = f\"D:/Data/final/merged/9_clothes/val\"\n",
    "\n",
    "dataset_val.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    "    classes=[\"clothes\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Test\n",
    "name_test = \"clothes_merge_test\"\n",
    "dataset_test = fo.Dataset(name_test)\n",
    "\n",
    "    # Coupang\n",
    "dataset_dir = f\"D:/Data/final/Coupang/631_dataset/clothes/test\"\n",
    "dataset_test.add_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "    # Amazon\n",
    "dataset_dir2 = f\"D:/Data/final/Amazon/631_dataset/clothes/test\"\n",
    "dataset_test.add_dir(\n",
    "    dataset_dir = dataset_dir2,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "    # Export\n",
    "export_dir = f\"D:/Data/final/merged/9_clothes/test\"\n",
    "\n",
    "dataset_test.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    "    classes=[\"clothes\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. class 모두 합치기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0% ||--------------|    1/1916 [18.1ms elapsed, 34.6s remaining, 55.3 samples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 1916/1916 [1.8s elapsed, 0s remaining, 1.0K samples/s]       \n",
      " 100% |███████████████| 1351/1351 [1.3s elapsed, 0s remaining, 1.0K samples/s]          \n",
      " 100% |███████████████| 2459/2459 [2.2s elapsed, 0s remaining, 1.1K samples/s]      \n",
      " 100% |███████████████| 2497/2497 [2.0s elapsed, 0s remaining, 1.3K samples/s]      \n",
      " 100% |███████████████| 2486/2486 [2.3s elapsed, 0s remaining, 1.0K samples/s]      \n",
      " 100% |███████████████| 2782/2782 [2.4s elapsed, 0s remaining, 1.2K samples/s]       \n",
      " 100% |███████████████| 2340/2340 [2.0s elapsed, 0s remaining, 1.2K samples/s]         \n",
      " 100% |███████████████| 2092/2092 [2.1s elapsed, 0s remaining, 1.0K samples/s]       \n",
      " 100% |█████████████| 17923/17923 [3.6m elapsed, 0s remaining, 61.1 samples/s]       \n"
     ]
    }
   ],
   "source": [
    "merge_classes = [\"dog_fence\", \"dog_nosework\", \"cat_tower\", \"cat_scratcher\", \"cat_toilet\", \"house\", \"carrier\", \"clothes\"]\n",
    "\n",
    "name_train = \"merge_train\"\n",
    "dataset_train = fo.Dataset(name_train)\n",
    "\n",
    "# dog_fence\n",
    "dataset_dir = f\"D:/Data/final/merged/2_dog_fence/train\"\n",
    "dataset_train.add_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "# dog_nosework\n",
    "dataset_dir2 = f\"D:/Data/final/merged/3_dog_nosework/train\"\n",
    "dataset_train.add_dir(\n",
    "    dataset_dir=dataset_dir2,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "# cat_tower\n",
    "dataset_dir3 = f\"D:/Data/final/merged/4_cat_tower/train\"\n",
    "dataset_train.add_dir(\n",
    "    dataset_dir=dataset_dir3,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "# cat_scratcher\n",
    "dataset_dir4 = f\"D:/Data/final/merged/5_cat_scratcher/train\"\n",
    "dataset_train.add_dir(\n",
    "    dataset_dir=dataset_dir4,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "# cat_toilet\n",
    "dataset_dir5 = f\"D:/Data/final/merged/6_cat_toilet/train\"\n",
    "dataset_train.add_dir(\n",
    "    dataset_dir=dataset_dir5,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "# house\n",
    "dataset_dir6 = f\"D:/Data/final/merged/7_house/train\"\n",
    "dataset_train.add_dir(\n",
    "    dataset_dir=dataset_dir6,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "# carrier\n",
    "dataset_dir7 = f\"D:/Data/final/merged/8_carrier/train\"\n",
    "dataset_train.add_dir(\n",
    "    dataset_dir=dataset_dir7,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "# clothes\n",
    "dataset_dir8 = f\"D:/Data/final/merged/9_clothes/train\"\n",
    "dataset_train.add_dir(\n",
    "    dataset_dir=dataset_dir8,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "    # Export\n",
    "export_dir = f\"D:/Data/final/merged/crawled_dataset/train\"\n",
    "\n",
    "dataset_train.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    "    classes=merge_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 958/958 [870.9ms elapsed, 0s remaining, 1.1K samples/s]       \n",
      " 100% |█████████████████| 675/675 [632.0ms elapsed, 0s remaining, 1.1K samples/s]      \n",
      " 100% |███████████████| 1229/1229 [1.1s elapsed, 0s remaining, 1.1K samples/s]         \n",
      " 100% |███████████████| 1248/1248 [1.0s elapsed, 0s remaining, 1.2K samples/s]         \n",
      " 100% |███████████████| 1243/1243 [1.2s elapsed, 0s remaining, 1.1K samples/s]         \n",
      " 100% |███████████████| 1391/1391 [1.2s elapsed, 0s remaining, 1.1K samples/s]          \n",
      " 100% |███████████████| 1169/1169 [1.1s elapsed, 0s remaining, 1.1K samples/s]         \n",
      " 100% |███████████████| 1046/1046 [926.0ms elapsed, 0s remaining, 1.1K samples/s]      \n",
      " 100% |███████████████| 8959/8959 [1.6m elapsed, 0s remaining, 126.8 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "merge_classes = [\"dog_fence\", \"dog_nosework\", \"cat_tower\", \"cat_scratcher\", \"cat_toilet\", \"house\", \"carrier\", \"clothes\"]\n",
    "\n",
    "name_val = \"merge_val\"\n",
    "dataset_val = fo.Dataset(name_val)\n",
    "\n",
    "# dog_fence\n",
    "dataset_dir = f\"D:/Data/final/merged/2_dog_fence/val\"\n",
    "dataset_val.add_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "# dog_nosework\n",
    "dataset_dir2 = f\"D:/Data/final/merged/3_dog_nosework/val\"\n",
    "dataset_val.add_dir(\n",
    "    dataset_dir=dataset_dir2,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "# cat_tower\n",
    "dataset_dir3 = f\"D:/Data/final/merged/4_cat_tower/val\"\n",
    "dataset_val.add_dir(\n",
    "    dataset_dir=dataset_dir3,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "# cat_scratcher\n",
    "dataset_dir4 = f\"D:/Data/final/merged/5_cat_scratcher/val\"\n",
    "dataset_val.add_dir(\n",
    "    dataset_dir=dataset_dir4,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "# cat_toilet\n",
    "dataset_dir5 = f\"D:/Data/final/merged/6_cat_toilet/val\"\n",
    "dataset_val.add_dir(\n",
    "    dataset_dir=dataset_dir5,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "# house\n",
    "dataset_dir6 = f\"D:/Data/final/merged/7_house/val\"\n",
    "dataset_val.add_dir(\n",
    "    dataset_dir=dataset_dir6,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "# carrier\n",
    "dataset_dir7 = f\"D:/Data/final/merged/8_carrier/val\"\n",
    "dataset_val.add_dir(\n",
    "    dataset_dir=dataset_dir7,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "# clothes\n",
    "dataset_dir8 = f\"D:/Data/final/merged/9_clothes/val\"\n",
    "dataset_val.add_dir(\n",
    "    dataset_dir=dataset_dir8,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "    # Export\n",
    "export_dir = f\"D:/Data/final/merged/crawled_dataset/val\"\n",
    "\n",
    "dataset_val.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    "    classes=merge_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████████| 322/322 [305.0ms elapsed, 0s remaining, 1.1K samples/s]       \n",
      " 100% |█████████████████| 227/227 [208.1ms elapsed, 0s remaining, 1.1K samples/s]     \n",
      " 100% |█████████████████| 413/413 [317.1ms elapsed, 0s remaining, 1.3K samples/s]      \n",
      " 100% |█████████████████| 418/418 [293.6ms elapsed, 0s remaining, 1.4K samples/s]      \n",
      " 100% |█████████████████| 415/415 [353.6ms elapsed, 0s remaining, 1.2K samples/s]      \n",
      " 100% |█████████████████| 465/465 [347.7ms elapsed, 0s remaining, 1.3K samples/s]      \n",
      " 100% |█████████████████| 392/392 [432.6ms elapsed, 0s remaining, 909.5 samples/s]     \n",
      " 100% |█████████████████| 349/349 [292.7ms elapsed, 0s remaining, 1.2K samples/s]      \n",
      " 100% |███████████████| 3001/3001 [30.4s elapsed, 0s remaining, 118.3 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "merge_classes = [\"dog_fence\", \"dog_nosework\", \"cat_tower\", \"cat_scratcher\", \"cat_toilet\", \"house\", \"carrier\", \"clothes\"]\n",
    "\n",
    "name_test = \"merge_test\"\n",
    "dataset_test = fo.Dataset(name_test)\n",
    "\n",
    "# dog_fence\n",
    "dataset_dir = f\"D:/Data/final/merged/2_dog_fence/test\"\n",
    "dataset_test.add_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "# dog_nosework\n",
    "dataset_dir2 = f\"D:/Data/final/merged/3_dog_nosework/test\"\n",
    "dataset_test.add_dir(\n",
    "    dataset_dir=dataset_dir2,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "# cat_tower\n",
    "dataset_dir3 = f\"D:/Data/final/merged/4_cat_tower/test\"\n",
    "dataset_test.add_dir(\n",
    "    dataset_dir=dataset_dir3,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "# cat_scratcher\n",
    "dataset_dir4 = f\"D:/Data/final/merged/5_cat_scratcher/test\"\n",
    "dataset_test.add_dir(\n",
    "    dataset_dir=dataset_dir4,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "# cat_toilet\n",
    "dataset_dir5 = f\"D:/Data/final/merged/6_cat_toilet/test\"\n",
    "dataset_test.add_dir(\n",
    "    dataset_dir=dataset_dir5,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "# house\n",
    "dataset_dir6 = f\"D:/Data/final/merged/7_house/test\"\n",
    "dataset_test.add_dir(\n",
    "    dataset_dir=dataset_dir6,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "# carrier\n",
    "dataset_dir7 = f\"D:/Data/final/merged/8_carrier/test\"\n",
    "dataset_test.add_dir(\n",
    "    dataset_dir=dataset_dir7,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "# clothes\n",
    "dataset_dir8 = f\"D:/Data/final/merged/9_clothes/test\"\n",
    "dataset_test.add_dir(\n",
    "    dataset_dir=dataset_dir8,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "    # Export\n",
    "export_dir = f\"D:/Data/final/merged/crawled_dataset/test\"\n",
    "\n",
    "dataset_test.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    "    classes=merge_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenImages + Crawled Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████| 5400/5400 [6.2s elapsed, 0s remaining, 972.6 samples/s]      \n",
      " 100% |█████████████| 17923/17923 [15.2s elapsed, 0s remaining, 1.0K samples/s]      \n",
      " 100% |█████████████| 23323/23323 [5.0m elapsed, 0s remaining, 136.3 samples/s]      \n",
      " 100% |███████████████| 1695/1695 [1.8s elapsed, 0s remaining, 930.6 samples/s]        \n",
      " 100% |███████████████| 8959/8959 [7.7s elapsed, 0s remaining, 1.1K samples/s]      \n",
      " 100% |█████████████| 10654/10654 [1.8m elapsed, 0s remaining, 122.2 samples/s]      \n",
      " 100% |█████████████████| 900/900 [1.1s elapsed, 0s remaining, 847.6 samples/s]         \n",
      " 100% |███████████████| 3001/3001 [2.6s elapsed, 0s remaining, 1.1K samples/s]      \n",
      " 100% |███████████████| 3901/3901 [41.4s elapsed, 0s remaining, 134.0 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "final_classes = [\"Dog\", \"Cat\", \"dog_fence\", \"dog_nosework\", \"cat_tower\", \"cat_scratcher\", \"cat_toilet\", \"house\", \"carrier\", \"clothes\"]\n",
    "\n",
    "# Train\n",
    "name_train = \"final_train\"\n",
    "dataset_train = fo.Dataset(name_train)\n",
    "\n",
    "    # openimages\n",
    "dataset_dir = f\"D:/Data/final/openimages/train/\"\n",
    "dataset_train.add_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "    # crawled_dataset\n",
    "dataset_dir2 = f\"D:/Data/final/merged/crawled_dataset/train/\"\n",
    "dataset_train.add_dir(\n",
    "    dataset_dir = dataset_dir2,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "    # Export\n",
    "export_dir = f\"D:/Data/final/dataset/train\"\n",
    "\n",
    "dataset_train.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    "    classes=final_classes\n",
    ")\n",
    "\n",
    "# Val\n",
    "name_val = \"final_val\"\n",
    "dataset_val = fo.Dataset(name_val)\n",
    "\n",
    "    # openimages\n",
    "dataset_dir = f\"D:/Data/final/openimages/val/\"\n",
    "dataset_val.add_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "    # crawled_dataset\n",
    "dataset_dir2 = f\"D:/Data/final/merged/crawled_dataset/val/\"\n",
    "dataset_val.add_dir(\n",
    "    dataset_dir = dataset_dir2,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "    # Export\n",
    "export_dir = f\"D:/Data/final/dataset/val/\"\n",
    "\n",
    "dataset_val.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    "    classes=final_classes\n",
    ")\n",
    "\n",
    "# Test\n",
    "name_train = \"final_test\"\n",
    "dataset_test = fo.Dataset(name_test)\n",
    "\n",
    "    # openimages\n",
    "dataset_dir = f\"D:/Data/final/openimages/test/\"\n",
    "dataset_test.add_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    ")\n",
    "    # crawled_dataset\n",
    "dataset_dir2 = f\"D:/Data/final/merged/crawled_dataset/test/\"\n",
    "dataset_test.add_dir(\n",
    "    dataset_dir = dataset_dir2,\n",
    "    dataset_type = fot.COCODetectionDataset,\n",
    ")\n",
    "\n",
    "    # Export\n",
    "export_dir = f\"D:/Data/final/dataset/test\"\n",
    "\n",
    "dataset_test.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fot.COCODetectionDataset,\n",
    "    classes=final_classes\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
